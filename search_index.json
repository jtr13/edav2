[["index.html", "edav.info, 2nd edition Chapter 1 Welcome!", " edav.info, 2nd edition Joyce Robbins 2023-11-04 Chapter 1 Welcome! This is the brand new start of edav.info 2.0! The first version of edav.info is still available, but will no longer be updated. With this resource, we try to give you a curated collection of tools and references that will make it easier to learn how to work with data in R. Detailed Examples will also be used to show proper exploratory data analysis under different circumstances. This resource is specifically tailored to the GR5293 Statistical Graphics and GR5702 Exploratory Data Analysis and Visualization courses offered at Columbia University. However, we hope that anyone interested in working with data in R will benefit from these pages. Happy coding! (Note. edav.info 2.0 is still under construction, and we will try our best to update new chapters weekly so that it will be up-to-date with the information you need to complete the current problem set.) "],["getting-started.html", "Chapter 2 Getting started 2.1 Top 10 essentials checklist 2.2 Troubleshooting 2.3 Tips &amp; tricks", " Chapter 2 Getting started Welcome to the world of EDAV! As you have already known, we will mainly use R through out the course. In an effort to get everyone on the same page, here is a checklist of essentials so you can get up and running. The best resources are scattered in different places online, so bear with links to various sites depending on the topic. 2.1 Top 10 essentials checklist (r4ds = R for Data Science by Garrett Grolemund and Hadley Wickham, free online) Install R (r4ds) – You need to have this installed but you won’t open the application since you’ll be working in RStudio. If you already installed R, make sure you’re current! The latest version of R (as of 2023-11-04) is R 4.3.2 “Eye Holes” released on 2023/10/31. (Use &gt; R.version to check what you have.) Install RStudio (r4ds) – Download the free, Desktop version for your OS. Working in this IDE will make working in R much more enjoyable. As with R, stay current. RStudio is constantly adding new features. The latest version (as of 2023-09-17) is 2023.06.2+561. (Click the RStudio menu, then “About RStudio” to see what version you have.) Get comfortable with RStudio – In this chapter of Bruno Rodriguez’s Modern R with the Tidyverse, you’ll learn about panes, options, getting help, keyboard shortcuts, projects, add-ins, and packages. Be sure to try out: Do some math in the console Create an Quarto file (.qmd) and render it to .html Install some packages like tidyverse or MASS Another great option for learning the IDE: Watch Writing Code in RStudio (RStudio webinar) Learn “R Nuts and Bolts” – Roger Peng’s chapter in R Programming will give you a solid foundation in the basic building blocks of R. It’s worth making the investing in understanding how R objects work now so they don’t cause you problems later. Focus on vectors and especially data frames; matrices and lists don’t come up often in data visualization. Get familiar with R classes: integer, numeric, character, and logical. Understand how factors work; they are very important for graphing. Tidy up (r4ds) – Install the tidyverse, and get familiar with what it is. We will discuss differences between base R and the tidyverse in class. Learn ggplot2 basics (r4ds) – In class we will study the grammar of graphics on which ggplot2 is based, but it will help to familiarize yourself with the syntax in advance. Avail yourself of the “Data Visualization with ggplot2” cheatsheet by clicking “Help” “Cheatsheets…” within RStudio. Learn some RMarkdown – For this class you will write assignments in R Markdown (stored as .Rmd files) and then render them into pdfs for submission. You can jump right in and open a new R Markdown file (File &gt; New File &gt; R Markdown…), and leave the Default Output Format as HTML. You will get a R Markdown template you can tinker with. Click the “knit” button and see what happens. For more detail, watch the RStudio webinar Getting Started with R Markdown. Update: RStudio has introduced Quarto, which they call “a multi-language, next generation version of R Markdown.” It is a separate application which must be installed independently. The syntax of Quarto is very similar to that of RMarkdown. Early adopters are welcome to try it out. Follow the installation instructions and tutorials on the get started page. A “Welcome to Quarto Workshop!” is available on YouTube, but it is long. Use RStudio projects (r4ds) – If you haven’t already, drink the Kool-Aid. Make each problem set a separate project. You will never have to worry about getwd() or setwd() again because everything will just be in the right places. Or watch the webinar: “Projects in RStudio”. If you run into a situation in which you must change the filepaths used to read files depending on whether you are running the code in the Console or knitting the document, it is likely due to having .Rmd files stored in subfolders of the project. The here package will eliminate the need for you to repeatedly make these changes by creating relative paths from the project root, that just work. This is a small but powerful tool; once you start using it there’s no going back. Learn the basic dplyr verbs for data manipulation (r4ds) – Concentrate on the main verbs: filter() (rows), select() (columns), mutate(), arrange() (rows), group_by(), and summarize(). Learn the native R pipe |&gt; operator. It is very similar to the magrittr pipe %&gt;% which you can see in action in the post “How dplyr replaced my most common R idioms”, which provides a detailed comparison of base R vs. dplyr data transformation. Know how to tidy your data – The pivot_longer() function from the tidyr package – successor to gather() – will help you get your data in the right form for plotting. More on this in class. Check out these super cool animations, which follow a data frame as it is transformed by tidyr functions. 2.2 Troubleshooting 2.2.1 Document doesn’t knit Normally an error message will display in the R Markdown section pointing to some lines with specific reasons. Try Googling as your first option and if not finding a solution, leave a post on ed discussion. 2.2.2 Functions stop working Strange behavior from functions that previously worked are often caused by function conflicts. This can happen if you have two packages loaded with the same function names. To indicate the proper package, namespace it. Conflicts commonly occur with select and filter and map. If you intend the tidyverse ones use: dplyr::select, dplyr::filter and purrr::map. Some other culprits: dplyr::summarise() and vcdExtra::summarise() ggmosaic::mosaic() and vcd::mosaic() leaflet::addLegend() and xts::addLegend() dplyr::select and MASS::select 2.3 Tips &amp; tricks 2.3.1 knitr Upon creating a new R markdown file, you should always notice a section like this: {r setup, include=False} knitr::opts_chunk$set(echo = TRUE) The chunk options refer to the first line and you can add some of the following options: {r setup, include=False, warning=False, message=False, cache=True} knitr::opts_chunk$set(echo = TRUE) warning=FALSE - Suppress warnings message=FALSE – Suppress messages, especially useful when loading packages cache=TRUE – only changed chunks will be evaluated, be careful though since changes in dependencies will not be detected. 2.3.2 Sizing figures Always use chunk options to size figures. You can set a default size in the YAML at the beginning of the .Rmd file as so: output: pdf_document: fig_height: 3 fig_width: 5 Another method is to click the gear ⚙️ next to the Knit button, then Output Options…, and finally the Figures tab. Then as needed override one or more defaults in particular chunks: {r, fig.width=4, fig.height=2} Figure related chunk options include fig.width, fig.height, fig.asp, and fig.align; there are many more. 2.3.3 R studio keyborad shortcuts Insert R chunk - option-command-i (Mac) - ctrl+alt+I (Windows) ```{r} ``` Insert %&gt;% (“the pipe”): shift-command(ctrl)-M Mac/Windows Comment/Uncomment lines shift-command(ctrl)-C Mac/Windows Knit Document shift-command(ctrl)-K Mac/Windows For more shortcuts, refer here 2.3.4 Viewing plots in plot window Would you like your plots to appear in the plot window instead of below each chunk in the .Rmd file? Click ⚙️ and then Chunk Output in Console. 2.3.5 Adding figures and links Add images ![DESCRIPTION HERE](PATH HERE) Add links: The text in the content column will act as a hyperlink [CONTENT HERE](LINK HERE) Note: Do not use these in r chunks as they will not work. "],["working-with-factors.html", "Chapter 3 Working with factors 3.1 Recode factor levels 3.2 Relevel the factor 3.3 Reorder the factors 3.4 Dealing with NAs 3.5 Summary of useful functions 3.6 Continuous to Categorical", " Chapter 3 Working with factors As what we have mentioned in the previous chapter, R sorts levels of factors in alphabetical order by default. In this chapter we will talk about working with factors using forcats package, which can be helpful when you managing categorical variables. 3.1 Recode factor levels Don’t directly assign levels with levels()&lt;-. Instead, using fct_recode(). library(forcats) x &lt;- factor(c(&quot;G234&quot;, &quot;G452&quot;, &quot;G136&quot;)) y &lt;- fct_recode(x, Physics = &quot;G234&quot;, Math = &quot;G452&quot;, Chemistry = &quot;G136&quot;) y ## [1] Physics Math Chemistry ## Levels: Chemistry Physics Math 3.2 Relevel the factor For the binned, ordinal data with levels out of order, fct_relevel() can be used to set a correct order. library(tibble) library(ggplot2) Births2015 &lt;- tibble(MotherAge = c(&quot;15-19 years&quot;, &quot;20-24 years&quot;, &quot;25-29 years&quot;, &quot;30-34 years&quot;, &quot;35-39 years&quot;, &quot;40-44 years&quot;, &quot;45-49 years&quot;, &quot;50 years and over&quot;, &quot;Under 15 years&quot;), Num = c(229.715, 850.509, 1152.311, 1094.693, 527.996, 111.848, 8.171, .754, 2.5)) ggplot(Births2015, aes(fct_relevel(MotherAge, &quot;Under 15 years&quot;), Num)) + geom_col() + coord_flip() + scale_y_continuous(breaks = seq(0, 1250, 250)) + ggtitle(&quot;United States Births, 2015&quot;, subtitle = &quot;in thousands&quot;) + theme_grey(16) + labs(y = &quot;mother age&quot;, x = &quot;count&quot;) The following examples give three circumstances when using fct_relevel(). Using fct_relevel() to move levels to the beginning: x &lt;- c(&quot;A&quot;, &quot;B&quot;, &quot;C&quot;, &quot;move1&quot;, &quot;D&quot;, &quot;E&quot;, &quot;move2&quot;, &quot;F&quot;) fct_relevel(x, &quot;move1&quot;, &quot;move2&quot;) ## [1] A B C move1 D E move2 F ## Levels: move1 move2 A B C D E F Using fct_relevel() to move levels after an item (by position): x &lt;- c(&quot;A&quot;, &quot;B&quot;, &quot;C&quot;, &quot;move1&quot;, &quot;D&quot;, &quot;E&quot;, &quot;move2&quot;, &quot;F&quot;) fct_relevel(x, &quot;move1&quot;, &quot;move2&quot;, after = 4) # move after the fourth item ## [1] A B C move1 D E move2 F ## Levels: A B C D move1 move2 E F Using fct_relevel() to move levels to the end x &lt;- c(&quot;A&quot;, &quot;B&quot;, &quot;C&quot;, &quot;move1&quot;, &quot;D&quot;, &quot;E&quot;, &quot;move2&quot;, &quot;F&quot;) fct_relevel(x, &quot;move1&quot;, &quot;move2&quot;, after = Inf) ## [1] A B C move1 D E move2 F ## Levels: A B C D E F move1 move2 If the row order is correct, use fct_inorder(): df &lt;- data.frame(temperature = factor(c(&quot;cold&quot;, &quot;warm&quot;, &quot;hot&quot;)), count = c(15, 5, 22)) # row order is correct (think: factor in ROW order) ggplot(df, aes(x = fct_inorder(temperature), y = count)) + geom_col() + theme_grey(16) + labs( x = &quot;temperature&quot;) 3.3 Reorder the factors Usually, unbinned, nominal data should be sorted by frequency order, which can be achieved using fct_infreq() (default is decreasing order of frequency) df &lt;- data.frame( color = c(&quot;orange&quot;,&quot;blue&quot;, &quot;red&quot;,&quot;brown&quot;,&quot;yellow&quot;, &quot;green&quot;, &quot;orange&quot;, &quot;red&quot;, &quot;yellow&quot;,&quot;blue&quot;,&quot;blue&quot;,&quot;red&quot;,&quot;orange&quot;,&quot;blue&quot;,&quot;red&quot;,&quot;orange&quot;,&quot;orange&quot;) ) ggplot(df, aes(fct_infreq(color))) + geom_bar() + theme_grey(16) For binned, nominal data which should be sorted by frequency order, use fct_reorder(). In the following example count is used, generally you can also apply mean,median, etc. to .fun inside `fct_reorder()``. pack1 &lt;- data.frame( color = c(&quot;blue&quot;, &quot;brown&quot;, &quot;green&quot;, &quot;orange&quot;, &quot;red&quot;, &quot;yellow&quot;), count = c(13, 7, 12, 9, 7, 8) ) ggplot(pack1, aes(fct_reorder(color, count, .desc = TRUE), count)) + geom_col() + theme_grey(16) + labs(x = &quot;color&quot;) 3.4 Dealing with NAs For prominent NA bars which should not be eliminated, use fct_explicit_na(x). And using fct_rev(x) to reverse the factor level doesn’t help. library(dplyr) df &lt;- data.frame(temperature = factor(c(&quot;cold&quot;, &quot;warm&quot;, &quot;hot&quot;, NA)), count = c(15, 5, 22, 12)) df %&gt;% mutate(temperature = fct_explicit_na(temperature, &quot;NA&quot;) %&gt;% fct_relevel(&quot;NA&quot;, &quot;hot&quot;, &quot;warm&quot;, &quot;cold&quot;)) %&gt;% ggplot(aes(x = temperature, y = count)) + geom_col() + coord_flip() + theme_grey(16) + labs(x = &quot;temperature&quot;) 3.5 Summary of useful functions For analyzing categorical variables, the first step is always to decide whether the class is ordinal or nominal. fct_recode(x, …) – change names of levels fct_inorder(x) – set level order of x to row order fct_relevel(x, …) – manually set the order of levels of x fct_reorder(x, y) – reorder x by y fct_infreq(x) – order the levels of x by decreasing frequency fct_rev(x) – reverse the order of factor levels of x fct_explicit_na(x) – turn NAs into a real factor level 3.6 Continuous to Categorical Sometimes you want to transfer a continuous variable to a categorical variable. For example, you might want assign grades to final scores of a course. In the following example, we generated a data set of test scores randomly and we assign grades based on some thresholds. We then apply function cut. (You can similarly use case_when) set.seed(2022) testscore &lt;- round(runif(100, min = 70, max = 100)) df &lt;- data.frame(testscore) |&gt; mutate(grade = cut(testscore, breaks = seq(70, 100, 10), labels = c(&quot;C&quot;, &quot;B&quot;, &quot;A&quot;), right = FALSE, include.lowest = TRUE)) head(df) ## testscore grade ## 1 94 A ## 2 89 B ## 3 74 C ## 4 86 B ## 5 76 C ## 6 89 B "],["data-importation.html", "Chapter 4 Data importation 4.1 Packages with built in data set 4.2 Data from web 4.3 Web scraping", " Chapter 4 Data importation Data is the core to any analysis. In this chapter, we will talk about various ways to import data into R. 4.1 Packages with built in data set Many of the packages in R have built in data sets. To gain access of the data, call the package and you can access them by the name of the data sets. For example, we can access the ExerciseHours data set from Lock5withR library(Lock5withR) head(ExerciseHours) ## Year Gender Hand Exercise TV Pulse Pierces ## 1 4 M l 15 5 57 0 ## 2 2 M l 20 14 70 0 ## 3 3 F r 2 3 70 2 ## 4 1 F l 10 5 66 3 ## 5 1 M r 8 2 62 0 ## 6 1 M r 14 14 62 0 4.2 Data from web 4.2.1 Read in from URL Some of the data you found online might be files ended with ‘.csv’ or ‘.xls’. In this case, you can directly read them using the URL. For example, X &lt;- read.csv(&quot;https://archive.ics.uci.edu/ml/machine-learning-databases/iris/iris.data&quot;, header = FALSE) #Read in iris data head(X) ## V1 V2 V3 V4 V5 ## 1 5.1 3.5 1.4 0.2 Iris-setosa ## 2 4.9 3.0 1.4 0.2 Iris-setosa ## 3 4.7 3.2 1.3 0.2 Iris-setosa ## 4 4.6 3.1 1.5 0.2 Iris-setosa ## 5 5.0 3.6 1.4 0.2 Iris-setosa ## 6 5.4 3.9 1.7 0.4 Iris-setosa Note that this is not a very stable way of reading data as the structure of the website might change, which will result in failure of reading the data. 4.2.2 API / R API package Some data sources provide APIs to access their data, for example CDC, Census, and Twitter. However, there is a learning curve in utilizing their APIs. Best practices for API packages will help you to get a head start. The other option is to find packages that handles the API calls for you. For example: CDC data - package wonderapi Census data - package censusapi Twitter - package rtweet A well-built client will save you a lot of time in retrieving data and should be your first resort. 4.3 Web scraping Web scraping is mostly considered as the last resort in obtaining data. As we put it in the last section, meaning that you should always explore the possibilities above before turning to web scraping. When scarping, you should think and investigate legal issues think about ethical questions limit bandwidth use scrape only what you need To start, you will need to know some backgrounds about the structure of a html page. In a webpage, you can always right click -&gt; inspect to check on the structure. Also, as a sanity check, we recommend using package robotstxt to see if scarping is allowed on a certain webpage. You can simply feed in the URL into function paths_allowed(). We will use the CRAN page for package forcats for the scraping example. library(robotstxt) library(rvest) paths_allowed(&#39;https://cran.r-project.org/web/packages/forcats/index.html&#39;) ## [1] TRUE 4.3.1 rvest Package rvest is widely used for web scraping. In the following example, read_html() takes the target webpage URL and html_table() extracts all table element in the page. forcats_data &lt;- read_html(&quot;https://cran.r-project.org/web/packages/forcats/index.html&quot;) forcats_table &lt;- forcats_data %&gt;% html_table() forcats_table[[1]] ## # A tibble: 13 × 2 ## X1 X2 ## &lt;chr&gt; &lt;chr&gt; ## 1 Version: &quot;1.0.0&quot; ## 2 Depends: &quot;R (≥ 3.4)&quot; ## 3 Imports: &quot;cli (≥ 3.4.0), glue, lifecycle, magrittr, rlang (≥ 1.0.0)… ## 4 Suggests: &quot;covr, dplyr, ggplot2, knitr, readr, rmarkdown, testthat (… ## 5 Published: &quot;2023-01-29&quot; ## 6 Author: &quot;Hadley Wickham [aut, cre],\\n RStudio [cph, fnd]&quot; ## 7 Maintainer: &quot;Hadley Wickham &lt;hadley at rstudio.com&gt;&quot; ## 8 BugReports: &quot;https://github.com/tidyverse/forcats/issues&quot; ## 9 License: &quot;MIT + file LICENSE&quot; ## 10 URL: &quot;https://forcats.tidyverse.org/,\\nhttps://github.com/tidyv… ## 11 NeedsCompilation: &quot;no&quot; ## 12 Materials: &quot;README NEWS&quot; ## 13 CRAN checks: &quot;forcats results&quot; If you want contents that are not in the table format, you can use html_nodes() specifying specific html structure. Namely, h2 tag - html_nodes(\"h2\") id attribute - html_nodes(\"#XXX\") class attribute - html_nodes(\".XXX\") "],["data-transformation.html", "Chapter 5 Data transformation 5.1 What is tidy data? 5.2 pivot_longer 5.3 pivot_wider 5.4 Basic transformation functions", " Chapter 5 Data transformation Plotting a graph is easy. You just need to find the right library with the right function. However, it is sometimes not so easy to get your data into the form desired to generate a graph. In this chapter, we will cover some basic techniques in tidying data with ggplot2. 5.1 What is tidy data? Here’s the definition of Tidy Data given by Hadley Wickham: A dataset is messy or tidy depending on how rows, columns and tables are matched up with observations, variables and types. In tidy data: Each variable forms a column. Each observation forms a row. Each observational unit forms a value in the table. See r4ds on tidy data for more info. What are the advantages of tidy data? Uniformity : It is easier to learn the tools that work with the data because they have a consistent way of storing data. Most built-in R functions work with vectors of values. Thus, having variables as columns/vectors allows R’s vectorized nature to shine. Take a look at the following data and can you tell whether this data is messy or not? ## mpg cyl disp hp drat wt qsec vs am gear carb ## Mazda RX4 21.0 6 160 110 3.90 2.620 16.46 0 1 4 4 ## Mazda RX4 Wag 21.0 6 160 110 3.90 2.875 17.02 0 1 4 4 ## Datsun 710 22.8 4 108 93 3.85 2.320 18.61 1 1 4 1 ## Hornet 4 Drive 21.4 6 258 110 3.08 3.215 19.44 1 0 3 1 ## Hornet Sportabout 18.7 8 360 175 3.15 3.440 17.02 0 0 3 2 ## Valiant 18.1 6 225 105 2.76 3.460 20.22 1 0 3 1 In this data set,all the variables are parameters of cars. This means that they are not different variables, but are values of a common variable. To transform the data, we use pivot_longer. 5.2 pivot_longer mtcars %&gt;% rownames_to_column(&quot;carname&quot;) %&gt;% pivot_longer(cols = !carname, names_to = &quot;Parameters&quot;,values_to = &quot;value&quot;) %&gt;% head() ## # A tibble: 6 × 3 ## carname Parameters value ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 Mazda RX4 mpg 21 ## 2 Mazda RX4 cyl 6 ## 3 Mazda RX4 disp 160 ## 4 Mazda RX4 hp 110 ## 5 Mazda RX4 drat 3.9 ## 6 Mazda RX4 wt 2.62 Follow the simple two steps: Identify the column you want to keep as is. In this case, we want all variables to match car names. Additionally, notice that in the original data set, carname acts as the index of the data set. You would want to convert the index to a column using rownames_to_column. Create meaningful names for the two new columns. In our case, straightforwardly, column names go into parameters and corresponding values go into value. 5.3 pivot_wider pivot_wider is just the opposite of pivot_longer. Using pivot_wider, we can transform our tidy data back into the messy form as all distinct values in Parameters will become column names. pivot_wider is often used in the case such that one observation being recorded over multiple rows. Consider the following example: ## # A tibble: 4 × 3 ## Country Type Number ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 USA Case 4 ## 2 USA Death 3 ## 3 Canada Case 2 ## 4 Canada Death 1 example %&gt;% pivot_wider(names_from = Type, values_from = Number) ## # A tibble: 2 × 3 ## Country Case Death ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 USA 4 3 ## 2 Canada 2 1 It would make much more sense if Case and Death are separate features. The main focus of this chapter is pivot_longer and pivot_wider. However, other fundamental functions in dplyr are also very important in manipulating your data set. In the following section, we will give an overview of the basics. 5.4 Basic transformation functions For the following sections, we will use data set biopsy from MASS for demonstration purpose. ## ID V1 V2 V3 V4 V5 V6 V7 V8 V9 class ## 1 1000025 5 1 1 1 2 1 3 1 1 benign ## 2 1002945 5 4 4 5 7 10 3 2 1 benign ## 3 1015425 3 1 1 1 2 2 3 1 1 benign ## 4 1016277 6 8 8 1 3 4 3 7 1 benign ## 5 1017023 4 1 1 3 2 1 3 1 1 benign ## 6 1017122 8 10 10 8 7 10 9 7 1 malignant 5.4.1 rename Upon getting the data, we noticed that the names of the columns are very vague. After reading the documentation, we wanted to change the names of the column so that the viewer gets a sense of the values they’re referring to. We use rename to modify the column names. biopsy_new &lt;- rename(biopsy, thickness = V1,cell_size = V2, cell_shape = V3, marg_adhesion = V4, epithelial_cell_size = V5, bare_nuclei = V6, chromatin = V7, norm_nucleoli = V8, mitoses = V9) head(biopsy_new) ## ID thickness cell_size cell_shape marg_adhesion epithelial_cell_size ## 1 1000025 5 1 1 1 2 ## 2 1002945 5 4 4 5 7 ## 3 1015425 3 1 1 1 2 ## 4 1016277 6 8 8 1 3 ## 5 1017023 4 1 1 3 2 ## 6 1017122 8 10 10 8 7 ## bare_nuclei chromatin norm_nucleoli mitoses class ## 1 1 3 1 1 benign ## 2 10 3 2 1 benign ## 3 2 3 1 1 benign ## 4 4 3 7 1 benign ## 5 1 3 1 1 benign ## 6 10 9 7 1 malignant 5.4.2 select select is column-wise operation. Specifically, only the columns that are specified will be returned. In the biopsy data, we do not require the variables “chromatin” and “mitoses”. So, let’s drop them using a minus sign: #selecting all except the columns chromatin and mitoses biopsy_new &lt;- biopsy_new %&gt;% dplyr::select(-chromatin,-mitoses) head(biopsy_new,5) ## ID thickness cell_size cell_shape marg_adhesion epithelial_cell_size ## 1 1000025 5 1 1 1 2 ## 2 1002945 5 4 4 5 7 ## 3 1015425 3 1 1 1 2 ## 4 1016277 6 8 8 1 3 ## 5 1017023 4 1 1 3 2 ## bare_nuclei norm_nucleoli class ## 1 1 1 benign ## 2 10 2 benign ## 3 2 1 benign ## 4 4 7 benign ## 5 1 1 benign 5.4.3 mutate The mutate function computes new variables from the already existing variables and adds them to the dataset. It gives information that the data already contained but was never displayed. The variable bare_nucleus contains the values from 1.00 to 10.00. If we wish to normalize the variable, we can use the mutate function: #normalize the bare nuclei values maximum_bare_nuclei&lt;-max(biopsy_new$bare_nuclei,na.rm=TRUE) biopsy_new &lt;- biopsy_new %&gt;% mutate(bare_nuclei=bare_nuclei/maximum_bare_nuclei) head(biopsy_new,5) ## ID thickness cell_size cell_shape marg_adhesion epithelial_cell_size ## 1 1000025 5 1 1 1 2 ## 2 1002945 5 4 4 5 7 ## 3 1015425 3 1 1 1 2 ## 4 1016277 6 8 8 1 3 ## 5 1017023 4 1 1 3 2 ## bare_nuclei norm_nucleoli class ## 1 0.1 1 benign ## 2 1.0 2 benign ## 3 0.2 1 benign ## 4 0.4 7 benign ## 5 0.1 1 benign In some situations, your new variable might involve conditions. You can consider using case_when combined with mutate. 5.4.4 filter filter is a row-wise operation. It returns a modified copy that contains only certain rows. This function filters rows based on conditions supplied in its argument. The filter function takes the data frame as the first argument. The next argument contains one or more logical tests. The rows/observations that pass these logical tests are returned in the result of the filter function. For our example, say we only want the data of those tumor cells that have clump thickness greater than 6. biopsy_new &lt;- biopsy_new %&gt;% filter(thickness&gt;5.5) head(biopsy_new,5) ## ID thickness cell_size cell_shape marg_adhesion epithelial_cell_size ## 1 1016277 6 8 8 1 3 ## 2 1017122 8 10 10 8 7 ## 3 1044572 8 7 5 10 7 ## 4 1047630 7 4 6 4 6 ## 5 1050670 10 7 7 6 4 ## bare_nuclei norm_nucleoli class ## 1 0.4 7 benign ## 2 1.0 7 malignant ## 3 0.9 5 malignant ## 4 0.1 3 malignant ## 5 1.0 1 malignant If you want to filter using multiple conditions, use logical operators: &amp;(And), |(Or). 5.4.5 arrange Arrange reorders the rows of the data based on their contents in the ascending order by default. Say in our example, the doctors would want to view the data in the order of the cell size of the tumor. #arrange in the order of V2:cell size head(arrange(biopsy_new,cell_size)) ## ID thickness cell_size cell_shape marg_adhesion epithelial_cell_size ## 1 1050718 6 1 1 1 2 ## 2 1204898 6 1 1 1 2 ## 3 1223967 6 1 3 1 2 ## 4 543558 6 1 3 1 4 ## 5 63375 9 1 2 6 4 ## 6 752904 10 1 1 1 2 ## bare_nuclei norm_nucleoli class ## 1 0.1 1 benign ## 2 0.1 1 benign ## 3 0.1 1 benign ## 4 0.5 10 malignant ## 5 1.0 7 malignant ## 6 1.0 4 malignant In case you want your data in descending order, wrap your variable with desc(). 5.4.6 group_by and summarize The summarize function uses the data to create a new data frame with the summary statistics such as minimum, maximum, average, and so on. These statistical functions must be aggregate functions which take a vector of values as input and output a single value. The group_by function groups the data by the values of the variables. This, along with summarize, makes observations about groups of rows of the dataset. The doctors would want to see the maximum cell size and the thickness for each of the classes: benign and malignant. This can be done by grouping the data by class and finding the maximum of the required variables: biopsy_grouped &lt;- group_by(biopsy_new,class) summarize(biopsy_grouped, max(thickness), mean(cell_size), var(norm_nucleoli)) ## # A tibble: 2 × 4 ## class `max(thickness)` `mean(cell_size)` `var(norm_nucleoli)` ## &lt;fct&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 benign 8 2.67 5.93 ## 2 malignant 10 6.73 11.3 5.4.7 slice_max (slice_min) The slice_max function helps you to find the top n values of a specific column. Suppose we now want to see the top five biopsies with the biggest thickness. Notice in this case, since we have more than five rows with thickness 10, all of them are selected (for neatness, we only show first several rows). biopsy_new %&gt;% slice_max(order_by = thickness,n=5) %&gt;% head() ## ID thickness cell_size cell_shape marg_adhesion epithelial_cell_size ## 1 1050670 10 7 7 6 4 ## 2 1054593 10 5 5 3 6 ## 3 1072179 10 7 7 3 8 ## 4 1080185 10 10 10 8 6 ## 5 1099510 10 4 3 1 3 ## 6 1103608 10 10 10 4 8 ## bare_nuclei norm_nucleoli class ## 1 1.0 1 malignant ## 2 0.7 10 malignant ## 3 0.5 4 malignant ## 4 0.1 9 malignant ## 5 0.3 5 malignant ## 6 0.1 10 malignant 5.4.8 join Sometimes you will need to combine two data sets and this is when function join comes into play. There are four types of joins provided by dplyr and take a look at the following example. # Main dataset s77 &lt;- data.frame(state.x77) %&gt;% rownames_to_column(&quot;state&quot;) %&gt;% dplyr::select(-c(Illiteracy)) head(s77) ## state Population Income Life.Exp Murder HS.Grad Frost Area ## 1 Alabama 3615 3624 69.05 15.1 41.3 20 50708 ## 2 Alaska 365 6315 69.31 11.3 66.7 152 566432 ## 3 Arizona 2212 4530 70.55 7.8 58.1 15 113417 ## 4 Arkansas 2110 3378 70.66 10.1 39.9 65 51945 ## 5 California 21198 5114 71.71 10.3 62.6 20 156361 ## 6 Colorado 2541 4884 72.06 6.8 63.9 166 103766 # https://www.cookpolitical.com/2020-national-popular-vote-tracker partyinfo &lt;- read_csv(&quot;https://docs.google.com/spreadsheets/d/e/2PACX-1vS3Z8Rq9xqOLISwoKdK0n6CFLBuPSCoXbbLeY8vhi-rzFS3ZFNEtR0BCdEbHcS-2Tlh5aPcnZbwBLao/pub?output=csv&quot;) partyinfo &lt;- partyinfo %&gt;% dplyr::select(state, called) head(left_join(s77, partyinfo)) ## state Population Income Life.Exp Murder HS.Grad Frost Area called ## 1 Alabama 3615 3624 69.05 15.1 41.3 20 50708 R ## 2 Alaska 365 6315 69.31 11.3 66.7 152 566432 R ## 3 Arizona 2212 4530 70.55 7.8 58.1 15 113417 D ## 4 Arkansas 2110 3378 70.66 10.1 39.9 65 51945 R ## 5 California 21198 5114 71.71 10.3 62.6 20 156361 D ## 6 Colorado 2541 4884 72.06 6.8 63.9 166 103766 D s77 contains statistics of 50 states in the US and partyinfo holds information whether a state is democratic or republican. The two data sets are joined on common feature state. If you want to join on features with different names, specify using the argument by. For detailed explanations of differnet types of joins, refer to the documentation. 5.4.9 Cases to tables When you want to perform a Chi-squared test or create a paired mosaic plot, your data has to follow a table format. For example, the following table is in the correct format. The columns are anxiety statues and rows are class years. ## moderate normal severe ## 1 11 34 2 ## 2 22 69 4 ## 3 8 41 5 ## 4 15 37 5 The following example demonstrates how you can convert cases to tables. Notice the starting data has a count for each of the categorical combinations. # watch out: summarise rebates &lt;- read_csv(&quot;https://data.ny.gov/resource/thd2-fu8y.csv&quot;) rebate_counts &lt;- rebates |&gt; group_by(make, ev_type) |&gt; summarize(Freq = n()) head(rebate_counts) ## # A tibble: 1 × 3 ## # Groups: make [1] ## make ev_type Freq ## &lt;chr&gt; &lt;chr&gt; &lt;int&gt; ## 1 Jeep PHEV 1000 By using xtabs, we are able to transform our data into a table ready for Chi-squared test or paired mosaic plot. head(xtabs(Freq ~ ., data = rebate_counts)) ## ev_type ## make PHEV ## Jeep 1000 "],["github.html", "Chapter 6 GitHub/git Resources 6.1 Overview 6.2 First things first 6.3 Usage 6.4 The no branch (besides main) workflow 6.5 Your repo with branching 6.6 1st PR on another repo with branching 6.7 2nd-nth PR on another repo with branching 6.8 Fixing mistakes 6.9 Troubleshooting 6.10 Other resources", " Chapter 6 GitHub/git Resources 6.1 Overview This section describes workflows for working with GitHub/git and advice on how to collaborate in teams on large coding projects. GitHub is super useful and powerful, but people also find it quite annoying and difficult to understand. We suggest taking it one step at a time, beginning with the basic workflows outlined below. You can derive great benefits from it without being an expert. 6.2 First things first Install Git. To do so, follow the instructions in the Install Git chapter of Happy Git with R. Tell git your name and email address. Introduce yourself to Git in Happy Git explains it all. Set up a personal access token. See Personal access token for HTTPS. (When you create the token, you will be given a choice of “Beta” or “classic”. Choose classic.) (Optional) Check that your setup works. See Connect RStudio to Git and GitHub. To be clear, you don’t have to do any connecting – the steps in this chapter check that it’s all working properly. 6.3 Usage Choose the right section based on what you’re trying to accomplish: If you are working by yourself and just getting started: the no branch workflow. If you are working by yourself and want to learn how to use branches, or are a collaborator (i.e. have write access) to another repo: your repo with branching. If you are not a collaborator (that is, don’t have write access to a repository) but wish to contribute to a project for the first time: 1st PR on another repo with branching. If you are not a collaborator but wish to contribute to the same project again: 2nd-nth PR on another repo with branching. 6.4 The no branch (besides main) workflow To get comfortable with Git, start with this basic workflow in which you will be pulling from and pushing to your repo on GitHub. Just you, no collaboration: The Connect RStudio to Git and GitHub chapter of Happy Git will get you set up: you will create a repo on GitHub, clone the repo into an RStudio project, and practice making changes. Once you’re set up, your local workflow will be pull, work, commit/push. PULL Each time you open RStudio and switch to the project, you will pull down any changes made to the repo on GitHub by clicking the Down Arrow in the Git pane of RStudio. You may think that no changes have been made to GitHub and there’s nothing to pull, but you may forget the typos that you fixed online, so it’s good practice to always start by pulling changes just in case. WORK Do your stuff. Make changes to files. Add new files. Keep an eye on the Git panel in RStudio; it will show you which files were changes. COMMIT/PUSH When you’re done working, you’ll want to think about what to do with the files that have changed. I like to keep the Git panel clear, so when I’m done I do one of three things with each file: 1) click “Staged” to get it ready for a sendoff to GitHub, 2) delete it if it’s not needed, 3) add it to .gitignore if it’s a file I want to keep locally but not send to GitHub. (Keep in mind that files in .gitignore are not backed up unless you have another backup system.) If I have a file that belongs somewhere else, I will move it there, so the only files left are the ones to send to GitHub. The next step is to click the Commit button and enter a commit message that meaningful describes what was done. Finally clicking the Up Arrow will send the commit to GitHub. It’s not considered good practice to commit too often, but as a beginner, it’s useful to do so to learn how it all works. 6.5 Your repo with branching Once you’ve comfortable with the workflow described above, you’re ready to start branching. If it’s your repo–or you have write access to someone else’s repo–begin by cloning the repo as in the workflow above. For emphasis: if you have write access to someone else’s repo, you should still clone the repo, not fork it. The repo will be referred to as “origin” even though you’re not the owner; origin simply means the repo from which the project was cloned. Next, continue with step 4 below, or follow the steps in these slides, which provide step-by-step detail on creating a branch, doing work on the branch and then submitting a pull request to merge the changes into origin/main. At any point, you can check the remotes (in this case, GitHub repositories) that are linked to your project with git remote -v. If your GitHub username is person1 and you have write access to a repo called finalproject created by person2, your remotes will look like this: $ git remote -v origin https://github.com/person2/finalproject.git(fetch) origin https://github.com/person2/finalproject.git(push) 6.6 1st PR on another repo with branching Step 1: Fork the upstream repo (once) Skip this step if you are syncing with a repo that you have write access to, whether it’s your own or someone else’s. Let’s say you want to contribute to edav.info! Fork our GitHub repo and then on your own GitHub page, you will see a forked edav2 repo under the repositories section. Note, from now on, the term upstream repository refers to the original repo of the project that you forked and the term origin repository refers to the repo that you created or forked on GitHub. From your respective, both upstream and origin are remote repositories. A fork of tidyverse/forcats Step 2: Clone origin and create a local repository (once) A local repository is the repo that resides on your computer. In order to be able to work locally, we need to create a local copy of the remote reposiotry. (For this to work you must first follow the instructions in First things first section.) On your GitHub repo page, copy the url of the origin repo by clicking on the green Code button and then the clipboard icon. It should look like this: https://github.com/jtr13/EDAV.git Then switch to RStudio, and click File -&gt; New Project -&gt; Version Control -&gt; Git. Now you can paste in the url of the origin repo and click Create Project to create a local repository. It is best to choose a location that is outside of other version control systems such as Dropbox to avoid conflicts. Step 3: Configure remote that points to the upstream repository (once) Skip this step if you’re syncing with a repo that you have write access to. The purpose of this step is to specify the location of the upstream repository, that is, the original project, not your copy of it. To complete this step, type in the following at the command line: $ git remote add upstream &lt;upstream repo url&gt; Source: Configuring a remote for a fork Once the upstream remote is added, you will have two remotes: origin and upstream. For example, the remotes for my (jtr13) local forcats repository are: $ git remote -v origin https://github.com/jtr13/forcats.git (fetch) origin https://github.com/jtr13/forcats.git (push) upstream https://github.com/tidyverse/forcats (fetch) upstream https://github.com/tidyverse/forcats (push) (Although four options are listed, that is, fetching or pushing from either remote, as the diagram above indicates, we will only fetch from upsteam and push to origin.) Step 4: Branch With this workflow, all new work is done on a branch, so it’s important to remember to create a new branch before you begin working. Once the work is complete, a pull request is submitted and if all goes well the new code will be merged into the main branch of the project on GitHub. When you’re ready to start working on something new, create a new branch. Do not reuse a merged branch. Each “fix” should get its own branch and be deleted after it’s been merged. To create a branch, click on the button shown below: Give your new branch a meaningful name. For example, if you intend to add a faceting example to the histogram chapter, you might call your branch add_hist_facet. Leave the “Sync branch with remote” box checked. Thereby you will not only create a local branch but also a remote branch on origin, and the local branch will be set up to track the remote branch. In short, they will be linked and git will take note of any changes on the other. Step 5: Work, commit and push When you create a branch following the method in Step 4, you will be automatically switched to the new branch. You can switch branches by clicking on the branch dropdown box to the right of the new branch button. However, be careful doing so. Work that isn’t committed, even if it is saved, doesn’t belong to a branch so it will move with you as you change branches. This makes it easy to accidentally be on the wrong branch. Check that you are in the right place and as you work keep an eye on changed files in the Git pane. Recall that there are three steps to moving saved work from your working directory to GitHub, represented by the git commands: add, commit, and push. In RStudio, to add, you simply click the checkbox for each file you have modified in the “staged” column on the left of the Git pane. To commit, you just click on the commit button under the Git tab. Entering a commit message is mandatory; choose a meaningful description of the code changes. Finally, to push changes to GitHub, click on the push button, which is represented by an upward pointing arrow. You can combine multiple commits into one “push”. It is not considered good practice to commit too enough because all the commits are entered into the commit history and it’s hard to find what you need if you commit your work every five minutes. (As you’re starting out, though, I wouldn’t be concerned about this. It’s more important to use the commands frequently to gain experience.) The Repeated Amend chapter of Happy Git with R describes one approach to dealing with the how-often-should-I-commit dilemma. Step 6: Submit a pull request Now you are able to see the branch you have created on the GitHub page. The next step is to submit a pull request and the process is very similar to the process described in the GitHub only walkthrough from the first version of edav.info, beginning with step 6. Step 7: Merge the pull request If you submitted a PR to another project, you are not the one who will be merging the pull request, so there’s nothing for you to do here. If it is your project, and it is your job to do so, be aware that there are many methods to merge a request. The most direct simple and direct is to merge the PR on GitHub. This method works well for merging fixed typos and the like. If you want to be able to test code, you may want to check out the PR locally, test it, and perhaps even make edits to it before merging. Best practices in this area are evolving. My current recommendation is to use the usethis package, which makes complex tasks very simple. “How to edit a pull request locally” explains how to do so. Another great resource is “Explore and extend a pull request” in Happy Git with R. This chapter describes two official GitHub versions of merging a pull request, as well as a workflow in development using git2r. 6.7 2nd-nth PR on another repo with branching After the first pull request, the process changes a little. We no longer need to fork and clone the repo. What we do need to do though is make sure that our local copy of the repository is up to date with the GitHub version. There is some other cleanup we need to do, so after the first pull request, we’ll replace steps #1 - #3 above with the following: Step 1: Sync How you sync depends on whether you are syncing with your own repo (“origin”) or someone else’s repo (“upstream”). This should be done at the beginning of every work session. Your repo Switch to the main branch (important!), then click the pull button (down arrow) in the Git pane in RStudio. Or you can type the following in the Terminal: $ git checkout main $ git pull There are no reminders that you’re behind, so it’s up to you. Make it a habit. Someone else’s repo If you’re working on someone else’s repo, make sure you’ve configured an upstream remote. Then do the following to update your fork: $ git fetch upstream $ git checkout main $ git merge upstream/main Source: Syncing a fork Note that these commands bring in changes directly from the upstream repo. If you are working on a branch and want to sync, check out that branch rather than main and fetch / merge. Switching branches can be performed by clicking on the appropriate branch in RStudio instead of git checkout. Step 2: Delete the old branch If your previous pull request was merged, it’s good practice to delete the associated branch since the upstream already contains all the changes you have made. To fully delete a branch you will need to 1) delete the local branch, 2) delete the remote branch, 3) stop tracking the branch: One way to delete the remote branch is to do so on GitHub. Navigate to the closed pull request on the upstream repo. If your branch has been merged, the pull request dialogue will display the following message: “You’re all set—the &lt;branchname&gt; branch can be safely deleted.” Simply click on the Delete branch button next to the message. If you prefer to work in the terminal, you can delete the remote branch with: $ git push origin --delete &lt;branchname&gt; To delete the local branch, switch to the main branch in RStudio and then type the following in the terminal: $ git branch -d branchname Take note that git doesn’t stop tracking the remote branch even though it’s gone in both places! To stop tracking deleted branches use the following: $ git fetch -p Otherwise you will still see the deleted branches listed in RStudio’s Git pane, and they will still appear when you look at all of your branches with $ git branch -a (* = checked out branch) Speaking of which, be aware that the Git pane doesn’t tend to update in real time, so you’ll likely still see branches listed that have been deleted. Be careful not to switch to them, or you will inadvertently recreate them. (Deleted branches have a habit of coming back.) Clicking on main (even though you’re already on main) appears to trigger an update of the dropdown list. If that doesn’t work, switching out of the project and back in will do so, if you want to be sure that the branches you deleted are really gone. Step 3: Update your fork on GitHub Skip this step if there’s no upstream repo. Yes, it’s odd, but once you’ve forked and cloned the project repo, the copy on GitHub becomes fairly irrelevant. However it’s not a bad idea to keep it up to date, if for no other reason than it’s disturbing to see messages like the following in your Git pane: Thankfully, your fork on GitHub can be updated easily by clicking the green up arrow or entering git push in the Terminal. Steps 4-7: See above Now we’re ready to repeat the branch, work, commit, push, submit a pull request workflow. To do so, follow Steps 4-7 above. 6.8 Fixing mistakes Fixing things generally involves returning to an earlier point in git history. To do so we need a way of referring to the point we want to return to. There are multiple ways of referring to the past. Most of the examples below use HEAD (the last commit) or HEAD~1 (the parent of the last commit, a.k.a. the 2nd to last commit). If you are contributing to someone else’s repo and things on your end get hopelessly messed up, the easiest way out is to start over. First be sure to keep a local copy of the files you need in a new folder, then delete your fork on GitHub, delete the local clone, fork again on GitHub, clone again to get a fresh local copy, and add the files you need back into the project. This is a variation of the “Burn it all down” described in Happy Git with R. 6.8.1 Forgot to branch if you didn’t commit anything yet: Just create the new branch and your work will be moved there, as changes in the working directory do not belong to a branch until they are committed. if you committed but didn’t push to GitHub: Undo the last commit with git reset --soft HEAD~1 (i.e. return to the parent commit). --soft means your changes won’t be erased. Then create the new branch. (Your work will move to it, see above.) if you committed and pushed to GitHub: Undo the last commit with git reset --soft HEAD~1, push to GitHub (you will need to use git push --force since the push will cause origin to lose commits, then create the new branch. (Your work will move to it, see above.) Note: Be very, very careful with --force as it is a dangerous tool. However, since the stakes are lower if you are force pushing to a fork, as would be the case if you are contributing to someone else’s repo, it’s ok to use it in this particular case. 6.8.2 Undoing stuff Undo the last commit: git reset --soft HEAD~1 (Fun fact: “How do I undo the most recent local commits in Git?” has the second highest number of votes of any question on Stack Overflow, and over 8 million views.) Undo changes since the last commit: git reset --hard HEAD Undo changes in one file since the last commit:git checkout -- [filename] SO link Undo deleted branch: Look for the SHA (hash) returned when you deleted the branch. Then:git checkout -b &lt;branch-name&gt; &lt;SHA&gt; SO link Remove all new, untracked files: git clean -f Remove all new, untracked files, including in new subdirectories: git clean -f -d Revert to a previous commit Using Git — how to go back to a previous commit 6.8.3 Random Make local the same as origin:git fetch origingit reset --hard origin/main SO link Add back a file that was deleted but still exists on another branch:git checkout otherbranch myfile.txt SO link Get rid of a file on GitHub that was added to .gitignore but is still there:git rm --cached [filename](then commit and push changes to GitHub) SO link Completely remove a folder from git history (use with caution) SO link 6.9 Troubleshooting 6.9.1 Deleting a branch isn’t working Make sure you’re not on the branch you’re trying to delete. Note that if you try to delete a branch that hasn’t been fully merged, you’ll get a warning, or perhaps an error depending on what’s transpired. It’s possible that it just thinks it hasn’t been merged even though it has, since you’re not up to date. This can be remedied with git pull. In other cases, you’ll need to follow the instructions to use -D instead of -d, for example, if you decide to abandon and delete a branch without submitting a pull request. If you have trouble getting rid of branches, rest assured, that you’re not alone. How do I delete a Git branch locally and remotely? is the third most asked question on StackOverflow! 6.10 Other resources Getting Help If you’re lost, these might help. GitHub Guides: This is a phenomenal collection of short articles from GitHub to help you learn about the fundamentals around their product. They are so great, we have already listed their Hello World article. Here are some other important ones: Understanding the GitHub Flow: Explains how working with GitHub generally goes. Git Handbook: Explains what version control is. GitHub Help: This is the yellow-pages of GitHub. Ask a question and it will try to push you in the right direction. Get it? Branching out GitHub is super social. Learn how to git involved! Open Source Guide: Info on how to contribute to open source projects. Great links to the GitHub skills involved as well as good GitHub etiquette to adopt. Forking Projects: Quick read from GitHub on how to fork a repository so you can contribute to it. Mastering Issues: On what Issues are in GitHub and how they can help get things done. Our Page on Contributing: You can contribute to edav.info/ with your new-found GitHub skills! Checkout our page on how to contribute through pull requests and/or issues. More Resources To hit the ground running, checkout GitHub Learning Lab. This application will teach you how to use GitHub with hands-on courses using actual repos. Its the perfect way to understand what using GitHub looks like. For the nerds in the room… Git For Ages 4 And Up: There’s a lot going on under the hood. This talk will help explain how it all works…with kids toys! Make pretty git logs: Always remember (A DOG). Also, this alias command is nice to have around: git config --global alias.adog \"log --all --decorate --oneline --graph\" add and commit with one command: Another (even more) helpful alias command: git config --global alias.add-commit '!git add -A &amp;&amp; git commit' Git Aware Prompt: An excellent add-on to the Terminal that informs you which branch you have checked out. Someone also made an even spiffier version where it will inform you of your git status using helpful emojis. Contributing with git2r, on the Population Genetics in R provides helpful information on using git commands within R through the git2r package. In particular it explains how to create a GITHUB_PAT and then set the credential parameter in certain functions to find the PAT. (Note though that the site was created in 2015 and as of February 2019 has not been updated.) Want a little reading as well?: Resources to learn Git is a simple site split into two main sections: Learn by reading and Learn by doing. Take your pick. "],["learning-ggplot2.html", "Chapter 7 Learning ggplot2 7.1 Getting started 7.2 Default part: layer 7.3 Customized parts 7.4 Resources for ggplot2 7.5 Required aesthetic mappings", " Chapter 7 Learning ggplot2 7.1 Getting started Hopefully, most of you already have experiences in plotting basic R graphics. In this Chapter, you will be briefly introduced one of the most powerful plotting packages in R: ggplot2 with it’s basic grammar and functions. To start, install ggplot2 in the console or in R chunk. install.packages(&#39;ggplot2&#39;) 7.2 Default part: layer For many R beginners, the question is always like: why is ggplot? One remarkable feature of ggplot2 is having an underlying grammar which enables you to compose graphs by combining different components. You can easily create novel graphics by adding ggplot2 functions to meet your needs based on your data. By definition of the grammar of graphics, the most important features are data and mapping in the layers and that’s where we are getting started. library(ggplot2) ggplot(data = iris) + #Data part geom_point(aes(Sepal.Length, Sepal.Width)) #Mapping part The most important part of all plots is data, which includes the information you want to visualize. Based on that, the next step is to decide its mapping, which determine how the data’s variable are mapped to aesthetic attributes on a graphic. Since data is independent from the other elements, you can always add several layers of data into the same ggplot while keeping the other components the same. ggplot(data = iris) + #Data part geom_point(aes(Petal.Length, Petal.Width)) + #layer 1 with mapping geom_point(aes(Sepal.Length, Sepal.Width), color=&#39;red&#39;) #layer 2 with a different mapping 7.3 Customized parts The following picture shows the order of ggplot functions: For more function order suggestions and auto-correction when writing your own ggplot2 functions, please refer to ggformat addin created by Joyce. 7.3.1 Geometric object, statistical transformation and position adjustment Geometric object, Statistical transformation and Position adjustment are components that can be customized in each layer. Geometric objects geoms control the type of plot you create. Different types of plot have different aesthetics features. For example, a point geom has position, color, shape, and size aesthetics. You should first decide which kind of plot better explains the data before choosing geoms and use help function to check what aesthetics can be modified to achieve your desired effects. A statistical transformation stat transforms the data. And Position adjustment is applied when you need to adjust the position of elements on the plot for dense data, otherwise data points might obscure one another. ggplot(data = iris) + geom_histogram(mapping=aes(x=Petal.Length, fill=Species), stat = &#39;bin&#39;,position = &#39;stack&#39;) 7.3.2 Scale A scale controls how data is mapped to aesthetic attributes, so one scale for one layer. ggplot(data = iris) + geom_histogram(mapping=aes(x=Petal.Length, fill=Species), stat = &#39;bin&#39;,position = &#39;stack&#39;) + scale_x_continuous(limits = c(0, 10)) + scale_y_continuous(limits = c(0, 50)) 7.3.3 Coordinate system A coordinate system coord maps the position of objects onto the plane of the plot, and controls how the axes and grid lines are drawn. One ggplot can only have one `coord`` ggplot(data = iris) + geom_histogram(mapping=aes(x=Petal.Length, fill=Species), stat = &#39;bin&#39;,position = &#39;stack&#39;) + coord_polar() 7.3.4 Faceting Faceting can be used to split the data up into subsets of the entire dataset. ggplot(data = iris) + geom_histogram(mapping=aes(x=Petal.Length), stat = &#39;bin&#39;) + facet_wrap(iris$Species) 7.3.5 Labels Labels include titles, labels for x,y axis and annotates. Good graphics also need to give clear information by using labels to tell readers’ of the background knowledge of your data. ggplot(data = iris) + geom_histogram(mapping=aes(x=Petal.Length, fill=Species), stat = &#39;bin&#39;,position = &#39;stack&#39;) + ggtitle(&#39;Stacked petal length of different species&#39;) + xlab(&#39;Length of Petal&#39;) 7.4 Resources for ggplot2 For more implementations and examples, one easiest way is referring to the ggplot2 Cheatsheets provided by R. Follow the steps shown below and you can find the cheat-sheets in your RStudio. The cheat-sheets clearly list the basic components of a ggplot where you can customize your unique plot by choosing different functions. If you are seeking for more detailed explanations and examples with real datasets, here are some useful links for you: ggplot2: Elegant Graphics ggformat 7.5 Required aesthetic mappings GEOM REQUIRED MAPPINGS geom_abline NA geom_area x and y geom_bar x or y geom_bin_2d x and y geom_bin2d x and y geom_blank NA geom_boxplot x or y geom_col x and y geom_contour x, y, and z geom_contour_filled x, y, and z geom_count x and y geom_crossbar x, y, ymin, and ymax or x, y, xmin, and xmax geom_curve x, y, xend, and yend geom_density x or y geom_density_2d x and y geom_density_2d_filled x and y geom_density2d x and y geom_density2d_filled x and y geom_dotplot x geom_errorbar x, ymin, and ymax or y, xmin, and xmax geom_errorbarh xmin, xmax, and y geom_freqpoly x or y geom_function NA geom_hex x and y geom_histogram x or y geom_hline yintercept geom_jitter x and y geom_label x, y, and label geom_line x and y geom_linerange x, ymin, and ymax or y, xmin, and xmax geom_map NA geom_path x and y geom_point x and y geom_pointrange x, y, ymin, and ymax or x, y, xmin, and xmax geom_polygon x and y geom_qq sample geom_qq_line sample geom_quantile x and y geom_raster x and y geom_rect xmin, xmax, ymin, and ymax geom_ribbon x, ymin, and ymax or y, xmin, and xmax geom_rug NA geom_segment x, y, xend, and yend geom_sf geometry geom_sf_label geometry geom_sf_text geometry geom_smooth x and y geom_spoke x, y, angle, and radius geom_step x and y geom_text x, y, and label geom_tile x and y geom_violin x and y geom_vline xintercept "],["faceting-1.html", "Chapter 8 Faceting 8.1 Faceting on one variable 8.2 Faceting on two variables", " Chapter 8 Faceting In this chapter, we will introduce facets, which are usually used to combine continuous and categorical data. 8.1 Faceting on one variable Facet partitions a plot into a matrix of panels. Each panel shows a different subset of the data. By default, facet_wrap gives consistent scales, which is easier for comparison between different panels. library(ggplot2) mycol = &quot;#7192E3&quot; ggplot(iris, aes(Sepal.Length, Sepal.Width)) + geom_point(color = mycol) + facet_wrap(~Species) + theme_grey(18) Rather than faceting on factor level, we can have one panel for each numerical variable. library(pgmm) library(dplyr) library(tidyr) data(wine) tidywine &lt;- wine %&gt;% pivot_longer(cols = -Type, names_to = &quot;variable&quot;, values_to = &quot;value&quot;) tidywine %&gt;% ggplot(aes(value)) + geom_histogram() + facet_wrap(~variable) + ggtitle(&quot;Consistent scales&quot;) + theme_grey(14) Axis scales can be made independent, by setting scales to free, free_x, or free_y. In this case, scales = \"free_x\" is a better option because the distribution of each numerical variable is more obvious. tidywine %&gt;% ggplot(aes(value)) + geom_histogram() + facet_wrap(~variable,scales = &quot;free_x&quot;) + ggtitle(&quot;Consistent scales&quot;) + theme_grey(14) 8.2 Faceting on two variables facet_grid can be used to split data-sets on two variables and plot them on the horizontal and/or vertical direction. wine %&gt;% mutate(Type = paste(&quot;Type&quot;, Type)) %&gt;% select(1:6) %&gt;% pivot_longer(cols = -Type, names_to = &quot;variable&quot;, values_to = &quot;value&quot;) %&gt;% ggplot(aes(value)) + geom_histogram(color = mycol, fill = &quot;lightblue&quot;) + facet_grid(Type ~ variable, scales = &quot;free_x&quot;) + theme_grey(14) "],["unidimensional-continuous-variables.html", "Chapter 9 Unidimensional continuous variables 9.1 Histogram 9.2 Boxplots 9.3 Ridgeline plot 9.4 Normal distribution", " Chapter 9 Unidimensional continuous variables In this chapter, we will demonstrate graphs with unidimensional continuous variables only using ggplot2. 9.1 Histogram 9.1.1 Basics and implications We will start with an easy example. library(ggplot2) library(gridExtra) #Example data x &lt;- c(50, 51, 53, 55, 56, 60, 65, 65, 68) #Stored as a dataframe df &lt;- data.frame(x) ggplot(df, aes(x)) + ggtitle(&quot;Histogram of x with ggplot2&quot;) + geom_histogram(color = &quot;blue&quot;, fill = &quot;lightBlue&quot;, binwidth = 5, center = 52.5) In this example, we used geom_histogram to create a histogram on variable x. We can see that it is quick to make and does not need much pre-processing. Moreover, Histograms show data’s empirical distribution within a set of intervals and we suggest using it as a one of the first steps to understand your data. Note: as shown above, ggplot expects a dataframe, so make sure you do not throw a vector into ggplot. 9.1.2 Types of histograms The y-scale of histograms can be represented in a variety of ways to express different results: Frequency or count: y = number of values that fall in each bin ggplot(finches, aes(x = Depth)) + geom_histogram(color=&quot;blue&quot;,fill=&quot;lightblue&quot;,binwidth = 0.5,boundary = 6) + ggtitle(&quot;Frequency histogram&quot;) Cumulative frequency: y = total number of values &lt;= (or &lt;) right boundary of bin ggplot(finches, aes(x = Depth)) + geom_histogram(aes(y = cumsum(after_stat(count))), color=&quot;blue&quot;, fill=&quot;lightblue&quot;, binwidth = 0.5, boundary = 6) + ggtitle(&quot;Cumulative frequency histogram&quot;) + xlab(&quot;Cumulative frequency&quot;) Density: y = relative frequency / binwidth ggplot(finches, aes(x = Depth)) + geom_histogram(aes(y = after_stat(density)), color = &quot;blue&quot;, fill = &quot;lightblue&quot;, binwidth = 0.5, boundary = 6) + ggtitle(&quot;Density histogram&quot;) 9.1.3 Parameters for geom_histogram 9.1.3.1 Bin boundaries Be mindful of the boundaries of the bins and whether a point will fall into the left or right bin if it is on a boundary. You can use the parameter closed to control the intervals. p1 &lt;- ggplot(df, aes(x)) + geom_histogram(color = &quot;blue&quot;, fill = &quot;lightblue&quot;, binwidth = 5, center = 52.5, closed = &quot;left&quot;) + ggtitle(&quot;Left closed graph&quot;) p2 &lt;- ggplot(df, aes(x)) + geom_histogram(color = &quot;blue&quot;, fill = &quot;lightblue&quot;, binwidth = 5, center = 52.5, closed=&quot;right&quot;) + ggtitle(&quot;Right closed graph&quot;) grid.arrange(p1, p2, ncol = 2) 9.1.3.2 Bin numbers #Default / Only adding some styles to make graph consistent ggplot(finches, aes(x = Depth)) + geom_histogram(color = &quot;blue&quot;, fill = &quot;lightblue&quot;) + ggtitle(&quot;Default with pop-up about bin number&quot;) We start by passing no parameters into geom_histogram and you will notice a pop-up saying that the default number of bins is 30. We see that the graph is not ideal with some gaps. There are two ways to modify the number of bins: specify the width explicitly with binwidth or provide the desired number of bins with bins. Consider the following modifications: # using binwidth p3 &lt;- ggplot(finches, aes(x = Depth)) + geom_histogram(color = &quot;blue&quot;, fill = &quot;lightblue&quot;, binwidth = 0.5, boundary = 6) + ggtitle(&quot;Changed binwidth value&quot;) # using bins p4 &lt;- ggplot(finches, aes(x = Depth)) + geom_histogram(color=&quot;blue&quot;, fill = &quot;lightblue&quot;, bins = 15, boundary = 6) + ggtitle(&quot;Changed bin value&quot;) # format plot layout grid.arrange(p3, p4, ncol = 2) Note: There is no gold standard on the number of bins, so try different numbers to generate best results. 9.1.3.3 Bin alignment Consider this comparison p5 &lt;- ggplot(finches, aes(x = Depth)) + geom_histogram(color = &quot;blue&quot;, fill = &quot;lightblue&quot;, binwidth = 0.5) + ggtitle(&quot;Without alignment&quot;) p6 &lt;- ggplot(finches, aes(x = Depth)) + geom_histogram(color = &quot;blue&quot;, fill = &quot;lightblue&quot;, bins = 15, boundary = 6) + ggtitle(&quot;With alignment&quot;) grid.arrange(p5, p6, ncol = 2) Notice that the boundary of bins does not start at an axis and the only difference in the code is the removal of boundry. To control the position of bins, we can use either parameter center or boundary. You can use boundary to specify the endpoint of any bin or center to specify the center of any bin and ggplot2 will be able to calculate where to place the rest of the bins. (Also, notice that when the boundary was changed, the number of bins got smaller by one. This is because by default the bins are centered and go over/under the range of the data.) In the above example, we specify boundary to be 6. We can see the first bin starts at 6 and the position of other bins are calculated based on the binwidth 0.5. 9.1.4 Interactive histograms with ggvis The ggvis package is not currently in development, but does certain things very well, such as adjusting parameters of a histogram interactively while coding. If you are interested, refer here. 9.2 Boxplots 9.2.1 Single boxplot A boxplot is one of the simplest ways of representing a distribution of a continuous variable (Never use boxplots for categorical data). It consists of two parts: box and whiskers. Let’s starting with a simple example: single boxplot. ggplot(chickwts, aes(x = weight)) + geom_boxplot() + ggtitle(&quot;Boxplot of chicken weights&quot;) Here as you can see, boxplots provide a ton of information for a single chart. Boxplots tell you whether the variable is normally distributed, or if the distribution is skewed in either direction. You can also easily spot the outliers, which always helps. Make a boxplot interactively (created with D3) 9.2.2 Multiple boxplots Next, what if you want to compare the distributions between multiple classes? Here, you can create a multiple boxplot. But remember, your data frame needs to be tidy, that is you need to have a column with levels of the grouping variable. It can be be factor, character, or integer class. The following example still use the chickwts dataset. We compare the distributions of weight between different feed(which is a column with six factor levels). ggplot(chickwts, aes(x = reorder(feed, -weight, median),y = weight)) + geom_boxplot() + ggtitle(&quot;Multiple boxplots of chicken weights according to feed type&quot;) + labs(y = &quot;Weight&quot;, x = &quot;Feed Type&quot;) Note. Usually in a boxplot, the boxes should be reordered so that there will be a decreasing order of the class medians from left to right. Often you want boxplots to be horizontal. Super easy to do in ggplot2: just tack on + coord_flip() and remove the - from the reordering so that the factor level with the highest median will be on top: ggplot(chickwts, aes(x = reorder(feed, weight, median),y = weight)) + geom_boxplot() + coord_flip() + ggtitle(&quot;Multiple boxplots of chicken weights according to feed type&quot;) + labs(y = &quot;Weight&quot;, x = &quot;Feed Type&quot;) 9.2.3 Additional resources Tukey, John W. 1977. Exploratory Data Analysis. Addison-Wesley. (Chapter 2): the primary source in which boxplots are first presented. Article on boxplots with ggplot2: An excellent collection of code examples on how to make boxplots with ggplot2. Covers layering, working with legends, faceting, formatting, and more. If you want a boxplot to look a certain way, this article will help. Boxplots with plotly package: boxplot examples using the plotly package. These allow for a little interactivity on hover, which might better explain the underlying statistics of your plot. ggplot2 Boxplot: Quick Start Guide: Article from STHDA on making boxplots using ggplot2. Excellent starting point for getting immediate results and custom formatting. Hadley Wickhan and Lisa Stryjewski on boxplots: good for understanding basics of more complex boxplots and some of the history behind them. 9.3 Ridgeline plot 9.3.1 Basics and implications Ridgeline plots can be used when a number of data segments have to be plotted on the same horizontal scale. It is presented with slight overlap. Ridgeline plots are very useful to visualize the distribution of a categorical variable over time or space. A good example using ridgeline plots will be a great example is visualizing the distribution of salary over different departments in a company. Consider the following example: library(ggridges) library(forcats) world &lt;- read.csv(&quot;countries2012.csv&quot;) ggplot(world, aes(x = GDP, y = reorder(CONTINENT, -GDP,median))) + geom_density_ridges(fill = &quot;blue&quot;) + ggtitle(&quot;2012 continental GDP&quot;) + ylab(&quot;Continent&quot;) ggridge uses two main geoms to plot the ridgeline density plots: geom_density_ridges and geom_ridgeline. They are used to plot the densities of categorical variable factors and see their distribution over a continuous scale. 9.3.2 Create better visuals ggplot(world, aes(x = GDP, y = reorder(CONTINENT, GDP,median))) + geom_density_ridges(fill = &quot;blue&quot;,alpha = .5, scale = 1.2) + ggtitle(&quot;2012 continental GDP&quot;) + ylab(&quot;Continent&quot;) In this example, we added parameter scale and alpha to control overlaps between ridges. Scale defines how much the peak of the lower curve touches the curve above and alpha controls transparency. Note that the curves are ordered from lowest median GDP on the bottom (Africa) to highest on the top (Europe). 9.3.3 Additional resources Introduction to ggridges: An excellent collection of code examples on how to make ridgeline plots with ggplot2. Covers every parameter of ggridges and how to modify them for better visualization. If you want a ridgeline plot to look a certain way, this article will help. Article on ridgeline plots with ggplot2: Few examples using different examples. Great for starting with ridgeline plots. History of Ridgeline plots: To refer to the theory of ridgeline plots. 9.4 Normal distribution When encountering data that seems to be normally distributed, you may want to overlay a normal curve. There are many ways to draw a normal curve and we introduce one here: ggplot(finches, aes(x = Depth)) + geom_histogram(aes(y = after_stat(density)), color = &quot;blue&quot;, fill = &quot;lightblue&quot;, binwidth = 0.5) + geom_function(fun = dnorm, col = &quot;red&quot;, args = list(mean(finches$Depth), sd(finches$Depth)), lwd = 1) + labs(title = &quot;Normal curve overlaid&quot;) In some situations you might want to draw separate normal curves after faceting on a categorical variable. Simply using stat_function will not generate the desired result. Consider the following examples, where normal curves were created for four plots using a single stat_function. As first glance, a normal curve appears in all of the plots. However, if you look closely, all the normal curves are actually the same one and generated on the whole dataset. In such situation, we suggest drawing each graph separately and combine them. "],["unidimensional-categorical-variables.html", "Chapter 10 Unidimensional categorical variables 10.1 Bar plot 10.2 Cleveland dot plot", " Chapter 10 Unidimensional categorical variables In real-world datasets, categorical features are quite common but tricky during both the data pre-processing and visualization process. In this chapter, we will demonstrate several plotting options for the uni-dimensional categorical variables with ggplot. 10.1 Bar plot There are two types of uni-dimensional categorical variables: nominal and ordinal. Here you will be shown how these variables should be plotted differently using bar plot under the same dataset. 10.1.1 Nominal data Nominal data is data with no fixed category order and should be sorted from highest to lowest count (left to right, or top to bottom) By default, R always sorts levels in alphabetical order. To reorder it by a sorted value, you can try fct_reorder , fct_rev, fct_relevel in the forcats package library(vcdExtra) library(ggplot2) library(forcats) library(dplyr) Accident %&gt;% group_by(mode) %&gt;% summarise(freq = sum(Freq)) %&gt;% ggplot(aes(x=fct_reorder(mode,freq,.desc = TRUE),y=freq)) + geom_bar(stat = &quot;identity&quot;,fill = &quot;cornflowerblue&quot;) + ggtitle(&quot;Number of people with different modes in accident&quot;) + xlab(&quot;&quot;) + theme(panel.grid.major.x = element_blank()) … or top to bottom Accident %&gt;% group_by(mode) %&gt;% summarise(freq = sum(Freq)) %&gt;% ggplot(aes(x=fct_rev(fct_reorder(mode,freq,.desc = TRUE)),y=freq)) + geom_bar(stat = &quot;identity&quot;,fill = &quot;cornflowerblue&quot;) + ggtitle(&quot;Number of people with different modes in accident&quot;) + coord_flip() + xlab(&quot;&quot;) + theme(panel.grid.major.x = element_blank()) 10.1.2 Ordinal data Ordinal data is data having a fixed category order and need to sort it in logical order of the categories (left to right) Accident %&gt;% group_by(age) %&gt;% summarise(freq = sum(Freq)) %&gt;% ggplot(aes(x=age,y=freq)) + geom_bar(stat = &quot;identity&quot;,fill = &quot;cornflowerblue&quot;) + ggtitle(&quot;Number of people of different ages in accident&quot;) + xlab(&quot;&quot;) + theme(panel.grid.major.x = element_blank()) Sort in logical order of the categories (starting at bottom OR top) Accident %&gt;% group_by(age) %&gt;% summarise(freq = sum(Freq)) %&gt;% ggplot(aes(x=age,y=freq)) + geom_bar(stat = &quot;identity&quot;,fill = &quot;cornflowerblue&quot;) + ggtitle(&quot;Number of people of different ages in accident&quot;) + xlab(&quot;&quot;) + coord_flip() + theme(panel.grid.major.x = element_blank()) 10.2 Cleveland dot plot Cleveland dot plot is a good alternative to bar plots, making plots more readable and comparable even with more data. Similarly, we also need to reorder the categorical variables just like what we’ve done for nominal bar plot. library(Lock5withR) ggplot(USStates, aes(x = IQ, y = fct_reorder(State, IQ))) + geom_point(color = &quot;blue&quot;) + ggtitle(&quot;Avg. IQ for US states&quot;) + ylab(&quot;&quot;) + theme_linedraw() 10.2.1 Cleveland dot plot with multiple dots Sort by Obese Rate library(tidyr) USStates %&gt;% select(&#39;State&#39;,&#39;Obese&#39;,&#39;HeavyDrinkers&#39;) %&gt;% gather(key=&#39;type&#39;,value=&#39;percentage&#39;,Obese,HeavyDrinkers) %&gt;% ggplot(aes(x=percentage, y=fct_reorder2(State,type==&#39;Obese&#39;,percentage,.desc=FALSE), color = type)) + geom_point() + ggtitle(&quot;Obese rate &amp; heavy drinker rate in US&quot;) + ylab(&quot;&quot;) + theme_linedraw() 10.2.2 Cleveland dot plot with facets You can split the graph into small multiples using facet_grid(). ggplot(USStates, aes(x = IQ, y = reorder(State, IQ))) + geom_point(color = &quot;blue&quot;) + facet_grid(Pres2008 ~ ., scales = &quot;free_y&quot;, space = &quot;free_y&quot;) + ggtitle(&#39;IQ of US state residents facet by Pres2008&#39;) + xlab(&quot;IQ&quot;) + ylab(&#39;&#39;) + theme_linedraw() + theme(panel.grid.major.x = element_blank(), panel.grid.minor.x = element_blank()) 10.2.3 Example: How Much People in the Trump Administration Are Worth # create dot plot theme theme_dotplot &lt;- theme_bw(16) + theme(axis.text.y = element_text(size = rel(.8)), axis.ticks.y = element_blank(), axis.title.x = element_text(), axis.text = element_text(face = &quot;bold&quot;), plot.background = element_rect(fill = &quot;lightcyan2&quot;), panel.background = element_rect(fill = &quot;moccasin&quot;), panel.grid.major.x = element_line(size = 0.5), panel.grid.major.y = element_line(size = 0.5, color = &quot;lightblue&quot;), panel.grid.minor.x = element_blank(), strip.text = element_text(size = rel(.7)), legend.position = &quot;top&quot;) # data source: # NYT, How Much People in the Trump Administration Are Worth # https://www.nytimes.com/interactive/2017/04/01/us/politics/how-much-people-in-the-trump-administration-are-worth-financial-disclosure.html df &lt;- read.csv(&quot;data/Assets.csv&quot;) # change units to millions df$Assets &lt;- df$Assets / 1000000 ggplot(df, aes(x = Assets, y = reorder(Name, Assets))) + geom_point() + ggtitle(&quot;How Much People in the Trump\\nAdministration Are Worth&quot;) + xlab(&quot;Assets in Millions $&quot;) + ylab(&quot;&quot;) + theme_dotplot # create Panel column df &lt;- df |&gt; mutate(Panel = cut(Assets, 4, breaks = fivenum(Assets), labels = c(&quot;$66k - $604k&quot;, &quot;$1 - 3.5 Million&quot;, &quot;$4 - 12 Million&quot;, &quot;$18 Million+&quot;))) |&gt; mutate(Panel = fct_rev(Panel)) ggplot(df, aes(x = Assets, y = reorder(Name, Assets))) + geom_point() + facet_wrap(~Panel, ncol = 1, scales = &quot;free&quot;) + ggtitle(&quot;How Much People in the Trump\\nAdministration Are Worth&quot;) + xlab(&quot;Assets in Millions $&quot;) + ylab(&quot;&quot;) + theme_dotplot "],["two-continuous-variables.html", "Chapter 11 Two continuous variables 11.1 Scatterplot 11.2 Heatmaps", " Chapter 11 Two continuous variables In this chapter, we will look at techniques that explore the relationships between two continuous variables. 11.1 Scatterplot 11.1.1 Basics and implications For the following example, we use data set SpeedSki. library(GDAdata) library(ggplot2) ggplot(SpeedSki, aes(Year, Speed)) + geom_point() + labs(x = &quot;Birth year&quot;, y = &quot;Speed achieved (km/hr)&quot;) + ggtitle(&quot;Skiers by birth year and speed achieved&quot;) In our example, we simply use geom_point on variables Year and Speed to create the scatterplot. we try to capture if there is a relationship between the age of a player and the speed he/she can achieve. From the graph, it seems such relationship does not exist. Overall, scatterplots are very useful in understanding the correlation (or lack thereof) between variables. The scatterplot gives a good idea of whether that relationship is positive or negative and if there’s a correlation. However, don’t mistake correlation in a scatterplot for causation! 11.1.2 Overplotting In some situations a scatter plot faces the problem of overplotting as there are so many points overlapping. Consider the following example from class. To save time, we randomly sample 20% of the data in advance. library(dplyr) library(ggplot2movies) sample &lt;- slice_sample(movies, prop = 0.2) ggplot(sample,aes(x=votes,y=rating)) + geom_point() + ggtitle(&quot;Votes vs. rating&quot;) + theme_classic() To create better visuals, we can use: Alpha blending - alpha=... Open circles - pch=21 smaller circles - size=... or shape=\".\" library(gridExtra) f1 &lt;- ggplot(sample,aes(x=votes,y=rating)) + geom_point(alpha=0.3) + theme_classic() + ggtitle(&quot;Alpha blending&quot;) f2 &lt;- ggplot(sample,aes(x=votes,y=rating)) + geom_point(pch = 21) + theme_classic() + ggtitle(&quot;Open circle&quot;) f3 &lt;- ggplot(sample,aes(x=votes,y=rating)) + geom_point(size=0.5) + theme_classic() + ggtitle(&quot;Smaller circle&quot;) grid.arrange(f1, f2, f3,nrow = 3) Other methods that directly deal with the data: Randomly sample data - as shown in the first code chunk using sample_n Subset - split data into bins using ntile(votes, 10) Remove outliers Transform to log scale 11.1.3 Interactive scatterplot You can create an interactive scatterplot using plotly. In the following example, we take 1% of the movie data set to present a better visual. We plotted the votes vs. rating and grouped by the year they are released. In this graph: You can hover on to the points to see the title of the movie You can double click on the year legend to look at a certain year You can zoom into a certain part of the graph to better understand the data points. library(plotly) sample2 &lt;- slice_sample(movies,prop=0.01) %&gt;% filter(year &gt; 2000) plot_ly(sample2, x = ~votes, y = ~rating, color = ~as.factor(year), text= ~title, hoverinfo = &#39;text&#39;) 11.1.4 Modifications 11.1.4.1 Contour lines Contour lines give a sense of the density of the data at a glance. For these contour maps, we will use the SpeedSki dataset. Contour lines can be added to the plot using geom_density_2d() and contour lines work best when combined with other layers ggplot(SpeedSki, aes(Year, Speed)) + geom_density_2d(bins=5) + geom_point() + ggtitle(&quot;Scatter plot with contour line&quot;) You can use bins to control the number of contour bins. 11.1.4.2 Scatterplot matrices If you want to compare multiple parameters to each other, consider using a scatterplot matrix. This will allow you to show many comparisons in a compact and efficient manner. For these scatterplot matrices, we use the movies dataset from the ggplot2movies package. As a default, the base R plot() function will create a scatterplot matrix when given multiple variables: sample3 &lt;- slice_sample(movies,prop=0.01) #sample data splomvar &lt;- sample3 %&gt;% dplyr::select(length, budget, votes, rating, year) plot(splomvar) While this is quite useful for personal exploration of a dataset, it is not recommended for presentation purposes. Something called the Hermann grid illusion makes this plot very difficult to examine. 11.2 Heatmaps 11.2.1 Basics and implications In the following example, we still use the SpeedSki data set. ggplot(SpeedSki, aes(Year, Speed)) + geom_bin2d() To create a heatmap, simply substitute geom_point() with geom_bin2d(). Generally, heat maps are like a combination of scatterplots and histograms: they allow you to compare different parameters while also seeing their relative distributions. 11.2.2 Modifications For the following section, we introduce some variations on heatmaps. 11.2.2.1 Change number of bins / binwidth By default, geom_bin2d() use 30 bins. Similar to a histogram, we can change the number of bins or binwidth. ggplot(SpeedSki, aes(Year, Speed)) + geom_bin2d(binwidth = c(5,5)) + ggtitle(&quot;Changing binwidth&quot;) Notice we are specifying the binwidth for both x and y axis. 11.2.2.2 Combine with a scatterplot ggplot(SpeedSki, aes(Year, Speed)) + geom_bin2d(binwidth = c(10, 10), alpha = .4) + geom_point(size = 2) + ggtitle(&quot;Combined with scatterplot&quot;) 11.2.2.3 Change color scale You can change the continuous scale of color ggplot(SpeedSki, aes(Year, Speed)) + geom_bin2d() + ggtitle(&quot;Changing color scale&quot;) + scale_fill_viridis_c() 11.2.2.4 Hex heatmap One alternative is a hex heatmap. You can create the graph using geom_hex ggplot(SpeedSki, aes(Year, Speed)) + geom_hex(binswidth = c(10,10)) + ggtitle(&quot;Hex heatmap&quot;) 11.2.2.5 Alternative approach to color If you look at all the previous examples, you might notice that lighter points correspond to more clustered points, which is somewhat counter-intuitive. The following example suggests an alternative approach in color scale. ggplot(SpeedSki, aes(Year, Speed)) + geom_hex(bins=12) + scale_fill_gradient(low = &quot;grey&quot;, high = &quot;purple&quot;) + theme_classic(18) + ggtitle(&quot;Alternative approach to color&quot;) "],["multidimensional-continuous-variables.html", "Chapter 12 Multidimensional continuous variables 12.1 Parallel coordinate plot 12.2 Biplot", " Chapter 12 Multidimensional continuous variables In this chapter, we will look at techniques that explore the relationships between multiple continuous variables. 12.1 Parallel coordinate plot 12.1.1 Basics and implications For the following example, we use the famous iris data set. After installing GGally, we use ggparcoord to create the plot simply by specifying the columns we want. library(GGally) ggparcoord(iris, columns=1:4, title = &quot;Parallel coordinate plot for Iris flowers&quot;) Generally, parallel coordinate plots are used to infer relationships between multiple continuous variables - we mostly use them to detect a general trend that our data follows, and also the specific cases that are outliers. Please keep in mind that parallel coordinate plots are not the ideal graph to use when there are just categorical variables involved. We can include a few categorical variables for the sake of clustering, but using a lot of categorical variables results in overlapping profiles, which makes it difficult to interpret. 12.1.2 Modifications The default parallel coordinate plot might be messy and hard to interpret. The following techniques will help to create better visuals and convey clearer trends. 12.1.2.1 Grouping Generally, you use grouping when you want to observe a pattern by group of a categorical variable. To do this, we set groupColumn to the desired categorical variable. 12.1.2.2 Alpha In practice, parallel coordinate plots are not going to be used for very small datasets. Your data will likely have thousands and thousands of cases, and sometimes it can get very difficult to observe anything when there are many overlaps. We set the alphaLines between zero and one, and it reduces the opacity of all lines. 12.1.2.3 Scales Sometimes the value in your variables have very different range and it is necessary to rescale them to make comparisons. By default, ggparcoord standardize your data. The following are some other scaling options: std: default value, where it subtracts mean and divides by standard deviation. robust: subtract median and divide by median absolute deviation. uniminmax: scale all values so that the minimum is at 0 and maximum at 1. globalminmax: no scaling, original values taken. 12.1.2.4 Splines Generally, we use splines if we have a column where there are a lot of repeating values, which adds a lot of noise. The case lines become more and more curved when we set a higher spline factor, which removes noise and makes for easier observations of trends. It can be set using the splineFactor attribute. 12.1.2.5 Reordering You can reorder your columns in any way you want. Simply put the order in a vector. For example: columns = c(1,3,4,2,5) 12.1.2.6 Application Consider the following example, we apply grouping, alpha tuning, scaling and splines on the iris data set. Compare the two plot and the modified graph is noticeably easier to interpret. ggparcoord(iris, columns=1:4, groupColumn=5, alpha=0.5, scale=&#39;uniminmax&#39;,splineFactor=10, title = &quot;Modified parallel coordinate plot for Iris flowers&quot;) 12.1.3 Interactive parallel coordinate plot Package parcoords can help us in creating interactive parallel coordinate plots. The following example is created using New York State crime data. df_a %&gt;% select(-c(&quot;Year&quot;,&quot;Months Reported&quot;,&quot;Index Total&quot;,&quot;Violent Total&quot;,&quot;Property Total&quot;)) %&gt;% arrange(df_a) %&gt;% parcoords(rownames = FALSE, brushMode = &quot;1D-axes&quot;, color = list(colorBy = &quot;Region&quot;, colorScale = &quot;scaleOrdinal&quot;, colorScheme = &quot;schemeCategory10&quot;), alpha = 0.5, withD3 = TRUE, width = 770, height = 600) In the interactive graph, for each feature, you can create a square box to filter for observations. For example, you can look at a certain county, or you can filter for all counties that are in New York City (Region=NYC). Overall, the interactive plot is more flexible for analysis. 12.1.4 External resource Just like a static graph, there is a lot of things you can change in the interactive setting. Refer to the Introduction to parcoords vignette for more options. Unfortunately, at the time of this writing the original blog post about the library is not available. 12.2 Biplot In the following chapter, we will introduce biplot. We will talk briefly on how to create a biplot and how to interpret it. 12.2.1 Principal components analysis (PCA) We first introduce PCA as the existence of biplot is built up on it. Given a data set with multiple variables, the goal of PCA is to reduce dimensionality by finding a few linear combinations of the variables that capture most of the variance. Consider the following example using rating of countries. As a common technique, we first standardize each variable to have mean of 0 and variance of 1 scaled_ratings &lt;- ratings %&gt;% mutate(across(where(is.numeric), ~round((.x-mean(.x))/sd(.x), 2))) scaled_ratings ## # A tibble: 13 × 7 ## country living_standard climate food security hospitality infrastructure ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Italy 0.9 1.04 1.2 0.5 -0.34 0.83 ## 2 Spain 0.9 1.49 1.2 0.5 -0.74 1.23 ## 3 Croatia -0.12 0.14 -0.03 1 0.47 0.43 ## 4 Brazil -0.12 1.04 0.38 -0.5 -0.74 -0.77 ## 5 Russia 0.39 -1.67 -1.68 -0.5 1.27 0.43 ## 6 Germany 1.41 -1.22 -1.68 2 1.27 1.63 ## 7 Turkey -0.12 1.04 1.2 -0.5 -1.15 -0.77 ## 8 Morocco -0.63 0.59 0.79 -1 -1.15 -1.17 ## 9 Peru -0.12 0.14 -0.03 -0.5 0.06 -0.37 ## 10 Nigeria -1.64 -0.76 -0.85 -1 -0.34 -1.17 ## 11 France 1.41 -0.76 0.38 1.5 2.08 1.23 ## 12 Mexico -1.64 -0.31 -0.44 -1 -0.34 -0.77 ## 13 SouthAfrica -0.63 -0.76 -0.44 -0.5 -0.34 -0.77 To apply PCA, we use function prcomp(). summary() will then be used to show result. pca &lt;- prcomp(ratings[,2:7], scale. = TRUE) summary(pca) ## Importance of components: ## PC1 PC2 PC3 PC4 PC5 PC6 ## Standard deviation 1.854 1.4497 0.43959 0.39052 0.27517 0.19778 ## Proportion of Variance 0.573 0.3503 0.03221 0.02542 0.01262 0.00652 ## Cumulative Proportion 0.573 0.9232 0.95544 0.98086 0.99348 1.00000 As we can see that the first two principal components capture 92.3% of the total variance. mat_round &lt;- function(matrix, n = 3) apply(matrix, 2, function(x) round(x, n)) mat_round(pca$rotation) ## PC1 PC2 PC3 PC4 PC5 PC6 ## living_standard -0.429 0.364 0.112 -0.673 0.466 -0.028 ## climate 0.270 0.585 -0.210 0.036 -0.149 -0.719 ## food 0.221 0.596 0.610 0.212 -0.077 0.417 ## security -0.475 0.244 -0.282 0.676 0.419 0.049 ## hospitality -0.484 -0.216 0.636 0.170 -0.213 -0.490 ## infrastructure -0.484 0.252 -0.297 -0.121 -0.731 0.256 We are also able to see the specific linear combination of variables for each principal component. 12.2.2 Draw a biplot To draw a biplot, we suggest using draw_biplot from redav package. You can install the package using remotes::install_github(\"jtr13/redav\"). Note that the function will apply PCA and draw the plot. library(redav) draw_biplot(ratings,arrows=FALSE) The above biplot is set to be without arrows. We can rougly identify clusters from the graph. By running some clustering algorithm like k-means, you will be able to see it clearer. scores &lt;- pca$x[,1:2] k &lt;- kmeans(scores, centers = 6) scores &lt;- data.frame(scores) %&gt;% mutate(cluster = factor(k$cluster), country = ratings$country) g4 &lt;- ggplot(scores, aes(PC1, PC2, color = cluster, label = country)) + geom_point() + geom_text(nudge_y = .2) + guides(color=&quot;none&quot;) g4 Now for a standard bibplot: draw_biplot(ratings) To interpret the graph, you could imagine a perpendicular line from a certain point(country) to a feature arrow you are concerned. The further the intersection is on the arrow line, the higher the score. Take Spain for example, it has high score on all variables except hospitality as the imaginary line would land on the negative axis. You can also add calibrated axis, which will help you better compare a certain variable among countries. draw_biplot(ratings,&quot;living_standard&quot;) You see in this case, a projection line is added. We can clearly see that France has the highest living standard rating and Nigeria has the lowest rating. "],["multidimensional-categorical-variables.html", "Chapter 13 Multidimensional categorical variables 13.1 Barcharts 13.2 Chi square test of independence 13.3 Mosaic plots 13.4 Alluvial diagrams 13.5 Heat map", " Chapter 13 Multidimensional categorical variables In this chapter, we will focus on multivariate categorical data. Here, it is noteworthy that multivariate plot is not the same as multiple variable plot, where the former is used for analysis with multiple outcomes. 13.1 Barcharts Bar chats are used to display the frequency of multidimensional categorical variables. In the next few plots you will be shown different kinds of bar charts. 13.1.1 Stacked bar chart library(ggplot2) library(dplyr) library(tidyr) cases &lt;- read.csv(&quot;data/icecream.csv&quot;) icecreamcolors &lt;- c(&quot;#ff99ff&quot;, &quot;#cc9966&quot;) # pink, coffee ggplot(cases, aes(x = Age, fill = Favorite)) + geom_bar() + scale_fill_manual(values = icecreamcolors) 13.1.2 Grouped bar chart Use position = \"dodge\" to create grouped bar chart ggplot(cases, aes(x = Age, fill = Favorite)) + geom_bar(position = &quot;dodge&quot;) + scale_fill_manual(values = icecreamcolors) 13.1.3 Grouped bar chart with facets ggplot(cases, aes(x = Age)) + geom_bar(position = &quot;dodge&quot;) + facet_wrap(~Favorite) 13.1.4 Grouped barchart with three categorical variables counts3 &lt;- cases %&gt;% group_by(Age, Favorite, Music) %&gt;% summarize(Freq = n()) %&gt;% ungroup() %&gt;% complete(Age, Favorite, Music, fill = list(Freq = 0)) ggplot(counts3, aes(x = Favorite, y = Freq, fill = Music)) + geom_col(position = &quot;dodge&quot;) + facet_wrap(~Age) 13.2 Chi square test of independence In this section, we would like to show how to use chi-square test to check the independence between two features. We will use the following example to answer: Are older Americans more interested in local news than younger Americans? The dataset is collected from here. local &lt;- data.frame(Age = c(&quot;18-29&quot;, &quot;30-49&quot;, &quot;50-64&quot;, &quot;65+&quot;), Freq = c(2851, 9967, 11163, 10911)) %&gt;% mutate(Followers = round(Freq*c(.15, .28, .38, .42)), Nonfollowers = Freq - Followers) %&gt;% select(-Freq) knitr::kable(local[,1:2]) Age Followers 18-29 428 30-49 2791 50-64 4242 65+ 4583 The chi-square hypothesis is set to be: Null hypothesis: Age and tendency to follow local news are independent Alternative hypothesis: Age and tendence to follow local news are NOT independent localmat &lt;- as.matrix(local[,2:3]) rownames(localmat) &lt;- local$Age X &lt;- chisq.test(localmat, correct = FALSE) X$observed ## Followers Nonfollowers ## 18-29 428 2423 ## 30-49 2791 7176 ## 50-64 4242 6921 ## 65+ 4583 6328 X$expected ## Followers Nonfollowers ## 18-29 984.1065 1866.893 ## 30-49 3440.4032 6526.597 ## 50-64 3853.2378 7309.762 ## 65+ 3766.2526 7144.747 X ## ## Pearson&#39;s Chi-squared test ## ## data: localmat ## X-squared = 997.48, df = 3, p-value &lt; 2.2e-16 We compare observed to expected and then the p-value tells that age and tendency are independent features. We are good to move on to next stage on mosaic plots. 13.3 Mosaic plots Mosaic plots are used for visualizing data from two or more qualitative variables to show their proportions or associations. 13.3.1 Mosaic plot with one variable library(grid) icecream &lt;- read.csv(&quot;data/MusicIcecream.csv&quot;) icecreamcolors &lt;- c(&quot;#ff99ff&quot;, &quot;#cc9966&quot;) counts2 &lt;- icecream %&gt;% group_by(Age, Favorite) %&gt;% summarize(Freq = sum(Freq)) vcd::mosaic(~Age, direction = &quot;v&quot;, counts2) 13.3.2 Mosaic plot with two variables vcd::mosaic(Favorite ~ Age, counts2, direction = c(&quot;v&quot;, &quot;h&quot;), highlighting_fill = icecreamcolors) 13.3.3 Mosaic plot with three variables(Best practice) Here’s some criteria of best practice of mosaic plots : Dependent variables is split last and split horizontally Fill is set to dependent variable Other variables are split vertically Most important level of dependent variable is closest to the x-axis and darkest (or most noticable shade) vcd::mosaic(Favorite ~ Age + Music, counts3, direction = c(&quot;v&quot;, &quot;v&quot;, &quot;h&quot;), highlighting_fill = icecreamcolors) 13.3.4 Mosaic pairs plot Use pairs method to plot a matrix of pairwise mosaic plots for class table: pairs(table(cases[,2:4]), highlighting = 2) 13.3.5 Mosaic plots: spine plot Spine plot is a mosaic plot with straight, parallel cuts in one dimension (“spines”) and only one variable cutting in the other direction. library(vcdExtra) library(forcats) foodorder &lt;- Alligator %&gt;% group_by(food) %&gt;% summarize(Freq = sum(count)) %&gt;% arrange(Freq) %&gt;% pull(food) ally &lt;- Alligator %&gt;% rename(Freq = count) %&gt;% mutate(size = fct_relevel(size, &quot;small&quot;), food = factor(food, levels = foodorder), food = fct_relevel(food, &quot;other&quot;)) vcd::mosaic(food ~ sex + size, ally, direction = c(&quot;v&quot;, &quot;v&quot;, &quot;h&quot;), highlighting_fill= RColorBrewer::brewer.pal(5, &quot;Accent&quot;)) 13.3.6 Mosaic plot: tree map Treemap is a filled rectangular plot representing hierarchical data (fill color does not necessarily represent frequency count) library(treemap) data(GNI2014) treemap::treemap(GNI2014, index=c(&quot;continent&quot;, &quot;iso3&quot;), vSize=&quot;population&quot;, vColor=&quot;GNI&quot;, type=&quot;value&quot;, format.legend = list(scientific = FALSE, big.mark = &quot; &quot;)) 13.4 Alluvial diagrams Alluvial diagrams are usually used to represent the flow changes in network structure over time or between different levels. The following plot shows the essential components of alluvial plots used in the naming schemes and documentation (axis, alluvium, stratum, lode): 13.4.1 ggalluvial library(ggalluvial) df2 &lt;- data.frame(Class1 = c(&quot;Stats&quot;, &quot;Math&quot;, &quot;Stats&quot;, &quot;Math&quot;, &quot;Stats&quot;, &quot;Math&quot;, &quot;Stats&quot;, &quot;Math&quot;), Class2 = c(&quot;French&quot;, &quot;French&quot;, &quot;Art&quot;, &quot;Art&quot;, &quot;French&quot;, &quot;French&quot;, &quot;Art&quot;, &quot;Art&quot;), Class3 = c(&quot;Gym&quot;, &quot;Gym&quot;, &quot;Gym&quot;, &quot;Gym&quot;, &quot;Lunch&quot;, &quot;Lunch&quot;, &quot;Lunch&quot;, &quot;Lunch&quot;), Freq = c(20, 3, 40, 5, 10, 2, 5, 15)) ggplot(df2, aes(axis1 = Class1, axis2 = Class2, axis3 = Class3, y = Freq)) + geom_alluvium(color=&#39;black&#39;) + geom_stratum() + geom_text(stat = &quot;stratum&quot;, aes(label = paste(after_stat(stratum), &quot;\\n&quot;, after_stat(count)))) + scale_x_discrete(limits = c(&quot;Class1&quot;, &quot;Class2&quot;, &quot;Class3&quot;)) You can choose to color the alluvium by different variables, for example, the first variable Class1 here: ggplot(df2, aes(axis1 = Class1, axis2 = Class2, axis3 = Class3, y = Freq)) + geom_alluvium(aes(fill = Class1), width = 1/12) + geom_stratum() + geom_text(stat = &quot;stratum&quot;, aes(label = paste(after_stat(stratum), &quot;\\n&quot;, after_stat(count)))) + scale_x_discrete(limits = c(&quot;Class1&quot;, &quot;Class2&quot;, &quot;Class3&quot;)) 13.4.2 geom_flow Another way of plotting alluvial diagrams is using geom_flow rather than geom_alluvium: ggplot(df2, aes(axis1 = Class1, axis2 = Class2, axis3 = Class3, y = Freq)) + geom_flow(aes(fill = Class1), width = 1/12) + geom_stratum() + geom_text(stat = &quot;stratum&quot;, aes(label = paste(after_stat(stratum), &quot;\\n&quot;, after_stat(count)))) + scale_x_discrete(limits = c(&quot;Class1&quot;, &quot;Class2&quot;, &quot;Class3&quot;)) After we use geom_flow, all Math students learning Art came together, which is also the same as Stats students. It makes the graph much clearer than geom_alluvium since there is less cross alluviums between each axises. 13.5 Heat map Besides what have been systematically introduced in Chapter 9.2 Heatmaps, this part demonstrated a special case of heat map when both x and y are categorical. Here the heat map can been seen as a clustered bar chart and a pre-defined theme is used to show the dense more clearly. library(vcdExtra) library(dplyr) theme_heat &lt;- theme_classic() + theme(axis.line = element_blank(), axis.ticks = element_blank()) orderedclasses &lt;- c(&quot;Farm&quot;, &quot;LoM&quot;, &quot;UpM&quot;, &quot;LoNM&quot;, &quot;UpNM&quot;) mydata &lt;- Yamaguchi87 mydata$Son &lt;- factor(mydata$Son, levels = orderedclasses) mydata$Father &lt;- factor(mydata$Father, levels = orderedclasses) mydata3 &lt;- mydata %&gt;% group_by(Country, Father) %&gt;% mutate(Total = sum(Freq)) %&gt;% ungroup() ggplot(mydata3, aes(x = Father, y = Son)) + geom_tile(aes(fill = (Freq/Total)), color = &quot;white&quot;) + coord_fixed() + scale_fill_gradient2(low = &quot;black&quot;, mid = &quot;white&quot;, high = &quot;red&quot;, midpoint = .2) + facet_wrap(~Country) + theme_heat "],["missing-data.html", "Chapter 14 Missing data 14.1 Introduction 14.2 Row / Column missing patterns", " Chapter 14 Missing data 14.1 Introduction Regardless what kind of data you have, missing data is a common issue. In this chapter, we will talk about missing data. Our focus will be on visualizing missing data and patterns to help you better understand your data. 14.2 Row / Column missing patterns Let’s consider the following two problem: Do all rows / columns have the same percentage of missing values? Are there correlations between missing rows / columns? (If a value is missing in one column it is likely to be missing in another column.) We will explore these two problems using mtcars with artificially generated missing values. library(dplyr) library(tibble) library(tidyr) library(ggplot2) library(forcats) set.seed(5702) mycars &lt;- mtcars mycars[,&quot;gear&quot;] &lt;- NA mycars[10:20, 3:5] &lt;- NA for (i in 1:10) mycars[sample(32,1), sample(11,1)] &lt;- NA 14.2.1 colSums() / rowSums() The most straightforward way to check missing values is using is.na() wrapped in colSums() / rowSums(). You will be able to observe the number of missing values column or row-wise. colSums(is.na(mycars)) %&gt;% sort(decreasing = TRUE) ## gear disp hp drat am cyl qsec vs mpg wt carb ## 32 12 12 11 3 1 1 1 0 0 0 #Show only the head rowSums(is.na(head(mycars))) %&gt;% sort(decreasing = TRUE) ## Mazda RX4 Mazda RX4 Wag Datsun 710 Hornet 4 Drive ## 1 1 1 1 ## Hornet Sportabout Valiant ## 1 1 14.2.2 Heatmap A graph can be more informative than plain numbers. If there are not a lot of rows or columns, we can use heatmaps to visualize the missing values. tidycars &lt;- mycars %&gt;% rownames_to_column(&quot;id&quot;) %&gt;% gather(key, value, -id) %&gt;% mutate(missing = ifelse(is.na(value), &quot;yes&quot;, &quot;no&quot;)) ggplot(tidycars, aes(x = key, y = fct_rev(id), fill = missing)) + geom_tile(color = &quot;white&quot;) + ggtitle(&quot;mtcars with NAs added&quot;) + ylab(&#39;&#39;) + scale_fill_viridis_d() + # discrete scale theme_bw() In the above example, we can clearly see where the missing values are for both rows and columns. Now, to add more information to the graph, consider the following example: tidycars &lt;- tidycars %&gt;% group_by(key) %&gt;% mutate(Std = (value-mean(value, na.rm = TRUE))/sd(value, na.rm = TRUE)) %&gt;% ungroup() ggplot(tidycars, aes(x = key, y = fct_rev(id), fill = Std)) + geom_tile(color = &quot;white&quot;) + ylab(&#39;&#39;) + scale_fill_gradient2(low = &quot;blue&quot;, mid = &quot;white&quot;, high =&quot;yellow&quot;, na.value = &quot;black&quot;) + theme_bw() In this graph, black represents missing values. The color scale from purple to yellow represents the magnitude of the values. 14.2.3 Patterns Other than the actual location of missing values, we can also explore the missing patterns. A Missing pattern refer to the combination of columns missing. The following picture should be a great demonstration of the concept. To explore the patterns, we use plot_missing() from redav. library(redav) plot_missing(mycars, percent = FALSE) Notice that there are three parts in the aggregated graph. The top part shows the number of missing values in each column. The middle part presents the missing patterns and the right part shows the counts for each missing patterns. You can also show the percentage in the top/right graphs by setting percentage = True. "],["colors.html", "Chapter 15 Colors 15.1 RColorBrewer 15.2 Perceptually uniform color spaces: Viridis", " Chapter 15 Colors When evaluating the power and efficiency of a plot, color is always a key factor that sometimes speaks a language even louder than words. So in this chapter, you will be introduced with several widely-applied color schemes and get to know how to use proper colors to make better plots based on different features of your data. 15.1 RColorBrewer RColorBrewer is an R package having built-in sensible color schemes ready-to-use for figures. Colors are grouped into three types: sequential, diverging, and qualitative. Sequential – Light colours for low data, dark for high data Qualitative(for categorical data) – Colours designed to give maximum visual difference between categories so great for non-ordered categorical data Diverging – Light colours for mid-range data, low and high use dark colours, great to seperate two extremes library(RColorBrewer) display.brewer.all() Here is an example of plotting categorical data using Dark2 pallets under qualitative group of RColorBrewer: library(ggplot2) ggplot(iris, aes(Petal.Length, Sepal.Length, colour = Species)) + geom_point() + scale_colour_brewer(palette = &quot;Dark2&quot;) Also, you can create your own sequential pallets. ggplot(faithfuld, aes(waiting, eruptions, fill = density)) + geom_raster() + scale_fill_gradient(low = &quot;white&quot;, high = &quot;red&quot;) Or diverging pallets: ggplot(faithfuld, aes(waiting, eruptions, fill = density)) + geom_raster() + scale_fill_gradient2(low = &quot;grey&quot;, mid = &quot;white&quot;, high = &quot;red&quot;,midpoint = .02) For discrete data, using scale_colour_manual is a good choice. For discrete ordinal data, we can use another package (such as vcd) ggplot(mtcars, aes(mpg, wt)) + geom_point(aes(colour = factor(cyl))) + scale_colour_manual(values = c(&quot;red&quot;, &quot;yellow&quot;, &quot;blue&quot;)) colors&lt;-brewer.pal(5,&#39;Blues&#39;) barplot(1:5, col=colors) 15.2 Perceptually uniform color spaces: Viridis The viridis R package provides four palettes for use in R which are pretty, perceptually uniform and easy to read by those with colorblindness. The package contains eight color scales: viridis, the primary choice, and five alternatives with similar properties - magma, plasma, inferno, civids, mako, and rocket -, and a rainbow color map - turbo. Perceived differences are proportional to scalar differences when using viridis. The following example shows viridison continuous data using scale_color_viridis_c, use scale_color_viridis_d() for discrete data library(&quot;viridis&quot;) ggplot(iris, aes(Sepal.Length, Sepal.Width))+ geom_point(aes(color = Sepal.Length)) + scale_color_viridis_c() "],["time-series.html", "Chapter 16 Time series 16.1 Dates 16.2 Time series 16.3 Multiple time series 16.4 Time series patterns 16.5 Index", " Chapter 16 Time series Time series, by definition, is a sequence of data point collected over a certain period of time. In this chapter, we will demonstrate several useful ways of plotting time-series data and how to processing date data type in R. 16.1 Dates Since time series analysis looks into how data is changing over time, the very first step is to transform the data into correct format. 16.1.1 Basic R functions You can convert character data to Date class with as.Date(): dchar &lt;- &quot;2018-10-12&quot; ddate &lt;- as.Date(dchar) class(dchar) ## [1] &quot;character&quot; class(ddate) ## [1] &quot;Date&quot; You can also specifying the format by: as.Date(&quot;Thursday, January 6, 2005&quot;, format = &quot;%A, %B %d, %Y&quot;) ## [1] &quot;2005-01-06&quot; For a list of the conversion specifications available in R, see ?strptime. Here is a list of the conversion specifications for date format from this post Also, Date class supports calculation between dates: as.Date(&quot;2017-11-02&quot;) - as.Date(&quot;2017-01-01&quot;) ## Time difference of 305 days as.Date(&quot;2017-11-12&quot;) &gt; as.Date(&quot;2017-3-3&quot;) ## [1] TRUE 16.1.2 Lubridate The tidyverse lubridate makes it easy to convert dates that are not in standard format with ymd(), ydm(), mdy(), myd(), dmy(), and dym() (among many other useful date-time functions): lubridate::mdy(&quot;April 13, 1907&quot;) ## [1] &quot;1907-04-13&quot; The lubridate package also provides additional functions to extract information from a date: today &lt;- Sys.Date() lubridate::year(today) ## [1] 2023 lubridate::yday(today) ## [1] 247 lubridate::month(today, label = TRUE) ## [1] Sep ## 12 Levels: Jan &lt; Feb &lt; Mar &lt; Apr &lt; May &lt; Jun &lt; Jul &lt; Aug &lt; Sep &lt; ... &lt; Dec lubridate::week(today) ## [1] 36 16.2 Time series For time-series data-sets, line plots are mostly used with time on the x-axis. Both base R graphics and ggplot2 “know” how to work with a Date class variable, and label the axes properly: The data comes from the official website. library(dplyr) library(readxl) library(tidyr) library(ggplot2) df &lt;- read_excel(&quot;data/historicalweeklydata.xls&quot;, col_types = c(&quot;date&quot;, &quot;numeric&quot;, &quot;numeric&quot;, &quot;numeric&quot;)) plot(df$Week, df$`30 yr FRM`, type = &quot;l&quot;) # on the order of years g&lt;-ggplot(df %&gt;% filter(Week &lt; as.Date(&quot;2006-01-01&quot;)), aes(Week, `30 yr FRM`)) + geom_line() + theme_grey(14) g We can control the x-axis breaks, limits, and labels with scale_x_date(), and use geom_vline() with annotate() to mark specific events in a time series. 16.3 Multiple time series The following plot shows a multiple time series of U.S. Mortgage rates. df2 &lt;- df %&gt;% pivot_longer(cols = -c(&quot;Week&quot;), names_to = &quot;TYPE&quot;) %&gt;% mutate(TYPE = forcats::fct_reorder2(TYPE, Week, value))# puts legend in correct order ggplot(df2, aes(Week, value, color = TYPE)) + geom_line() + ggtitle(&quot;U.S. Mortgage Rates&quot;) + labs (x = &quot;&quot;, y = &quot;percent&quot;) + theme_grey(16) + theme(legend.title = element_blank()) To plot the time series in a specific period of time, use filter() before ggplot: library(lubridate) df2010 &lt;- df2 %&gt;% filter(year(Week) == 2010) ggplot(df2010, aes(Week, value, color = TYPE)) + geom_line() + ggtitle(&quot;U.S. Mortgage Rates&quot;) 16.4 Time series patterns Next, as an important part of time series analysis, we want to find the existing patterns of the data. We first starting from plotting the overall long-term trend: library (readr) urlfile=&quot;https://raw.githubusercontent.com/jtr13/data/master/ManchesterByTheSea.csv&quot; data&lt;-read_csv(url(urlfile)) g &lt;- ggplot(data, aes(Date, Gross)) + geom_line() + ggtitle(&quot;Manchester by the Sea&quot;, &quot;Daily Gross (US$), United States&quot;) + xlab(&quot;2016-2017&quot;) g Adding a smoother to the data, adjusting the smoothing parameter span = to find a proper smoother which is not overfitting/underfitting: g &lt;- ggplot(data, aes(Date, Gross)) + geom_point() g + geom_smooth(method = &quot;loess&quot;, span = .5, se = FALSE) Mark the pattern by high-lighting the data on very Saturday: g &lt;- ggplot(data, aes(Date, Gross)) + geom_line() + ggtitle(&quot;Manchester by the Sea&quot;, &quot;Daily Gross, United States&quot;) saturday &lt;- data %&gt;% filter(wday(Date) == 7) g + geom_point(data = saturday, aes(Date, Gross), color = &quot;deeppink&quot;) Another way is to use facet to show the cyclical pattern: ggplot(data, aes(Date, Gross)) + geom_line(color = &quot;grey30&quot;) + geom_point(size = 1) + facet_grid(.~wday(Date, label = TRUE)) + geom_smooth(se = FALSE) Also, basic R can plot the decomposed time series automatically. This method is used to study the trend, seasonal effect on data with at least 2 periods. For additive components, use type = \"additive\". tsData &lt;- EuStockMarkets[, 2] decomposedRes &lt;- decompose(tsData, type=&quot;mul&quot;) plot (decomposedRes) 16.5 Index When making comparisons on multi-line plots, index is a way of scaling the data: Each value is divided by the first value for that group and multiplied by 100. urlfile=&quot;https://raw.githubusercontent.com/jtr13/data/master/WA_Sales_Products_2012-14.csv&quot; sale&lt;-read_csv(url(urlfile)) sale$Q &lt;- as.numeric(substr(sale$Quarter, 2, 2)) # convert Q to end-of-quarter date sale$Date &lt;- as.Date(paste0(sale$Year, &quot;-&quot;,as.character(sale$Q*3),&quot;-30&quot;)) Methoddata &lt;- sale %&gt;% mutate(Revenue = Revenue/1000000) %&gt;% group_by(Date,`Order method type`) %&gt;% summarize(Revenue = sum(Revenue)) %&gt;% ungroup() %&gt;% group_by(`Order method type`) %&gt;% mutate(index = round(100*Revenue/Revenue[1], 2)) %&gt;% ungroup() g &lt;- ggplot(Methoddata, aes(Date, index,color = `Order method type`)) + geom_line(aes(group = `Order method type`)) + scale_x_date(limits = c(as.Date(&quot;2012-02-01&quot;), as.Date(&quot;2014-12-31&quot;)), date_breaks = &quot;6 months&quot;, date_labels = &quot;%b %Y&quot;) g "],["spatial-data.html", "Chapter 17 Spatial data 17.1 Introduction 17.2 Packages 17.3 Shape file", " Chapter 17 Spatial data The page is currently being updated, check back later. 17.1 Introduction In this chapter, we will take a glimpse into spatial analysis using R. There are an overwhelming number of R packages for analyzing and visualizing spatial data. In broad terms, spatial visualizations require a merging of non-spatial and spatial information. For example, if you wish to create a choropleth map of the murder rate by county in New York State, you need county level data on murder rates, and you also need geographic data for drawing the county boundaries, stored in what are called shape files. A rough divide exists between packages that don’t require you deal with shape files and those that do. The former work by taking care of the geographic data under the hood: you supply the data with a column for the location and the package takes care of figuring out how to draw those locations. 17.2 Packages 17.2.1 choroplethr Choropleth maps use color to indicate the value of a variable within a defined region, generally political boundaries. choroplethr is capable of drawing state and county level maps without using shape files. Consider the following example using state.x77 showing the percentage of illiterate in each states. Note the process of data transformation, you need to exactly have a column named ‘region’ with the state names and a column named ‘value’. library(dplyr) library(tidyr) library(tibble) library(ggplot2) library(choroplethr) # data frame must contain &quot;region&quot; and &quot;value&quot; columns df_illiteracy &lt;- state.x77 %&gt;% as.data.frame() %&gt;% rownames_to_column(&quot;state&quot;) %&gt;% transmute(region = tolower(`state`), value = Illiteracy) state_choropleth(df_illiteracy, title = &quot;State Illiteracy Rates, 1977&quot;, legend = &quot;Percent Illiterate&quot;) Plotting at county level is also possible. In the following example we show the population of counties in New York state in 2012. library(choroplethrMaps) data(county.regions) data(df_pop_county) ny_county_fips = county.regions %&gt;% filter(state.name == &quot;new york&quot;) %&gt;% select(region) county_choropleth(df_pop_county, title = &quot;Population of Counties in New York State in 2012&quot;, legend = &quot;Population&quot;, county_zoom = ny_county_fips$region) We use county_choropleth to create a U.S map at county level and zoom into New York state. Note that county_zoom takes in fips codes of counties. For more reference, visit the R documentation page. 17.2.2 ggmap ggmap is a great package to generate real-world maps. Moreover, it is compatible with ggplot2 allowing you to easily add layers on top of the base map. There are two options in generating maps and we demonstrate one using stamenmap. library(ggmap) ny_map &lt;- get_stamenmap(bbox = c(left = -74.2591, bottom = 40.4774, right = -73.7002, top = 40.9162), zoom = 10, maptype = &quot;toner-lite&quot;) ggmap(ny_map) + geom_point(aes(x=-73.9857,y=40.7484),size=0.8,color=&#39;blue&#39;) A map of New York City is created, and you will see a blue point representing Empire State Building. You can very simply add points using geom_point as long as you have the coordinates for the points. However, one clear drawback using stamenmap is that the map resolution is not satisfying and you will eventually find that the smaller zoom value creates blurry maps. For this reason, we encourage readers to try the option using Google maps. You will need to set up a Google Cloud account (requires credit card). After enabling the Google map APIs, register you API keys with ggmap using register_google(key = \"[your key]\") and you are all set. Now take a look at the an example using Google map API ggmap(get_googlemap(center = c(lon = -74.006, lat = 40.7128),zoom = 14)) You will see the map is at great resolution regardless of how you zoom in. For demonstration purpose, we provided a image of the map derived by running the code. For more details regarding the package usage, you can refer at ggmap GitHub page. 17.3 Shape file There are cases such that no existing packages can directly create your desired map. For example, what if we want to create a map of highways in New York State. In such case, we will resort to shape files. In the following example, we use the shape file of police precinct in New York City. 17.3.1 sf We use sf library to read in shape files. One of the first thing to remember is that one shape file contains a set of files and all of them required for the shape file to run properly. However, when reading the file, we only need to use the one ending with .shp. For example library(sf) ny_police = st_read(&#39;nyc_police/nypp.shp&#39;,quiet=TRUE) %&gt;% mutate(Precinct = as.character(Precinct)) head(ny_police) ## Simple feature collection with 6 features and 3 fields ## Geometry type: MULTIPOLYGON ## Dimension: XY ## Bounding box: xmin: 971013.5 ymin: 188082.3 xmax: 992119.1 ymax: 217541.3 ## Projected CRS: NAD83 / New York Long Island (ftUS) ## Precinct Shape_Leng Shape_Area geometry ## 1 1 81117.86 47300568 MULTIPOLYGON (((972081.8 19... ## 2 5 18807.12 18094527 MULTIPOLYGON (((987399.2 20... ## 3 6 26413.24 22103327 MULTIPOLYGON (((984337.5 20... ## 4 7 17288.06 18365996 MULTIPOLYGON (((991608.6 20... ## 5 9 19773.00 21395839 MULTIPOLYGON (((992119.1 20... ## 6 10 40281.97 27266581 MULTIPOLYGON (((983866 2172... You can see that the object looks like a data set, except with a long column called geometry. The geometry column contains spatial information and is the key to map generation. To simply see the shape, we can use st_geometry() to extract spatial information and feed into plot() plot(st_geometry(ny_police)) We get a map of New York City divided by police precinct. Note that if you directly feed in ny_police into plot(), you will get mulitiply maps drawn from each column. It might not create you desired maps and when you are dealing with large shape files, it will be extremely slow. 17.3.2 tmap Only generating the map is not particularly meaningful. We want to combine data with the map to convey some meaningful findings. Since we are using police precinct, it is only natural to use crime data. library(readxl) library(tmap) ny_crime &lt;- read_xls(&#39;data/felony.xls&#39;, skip = 1) %&gt;% rename(Precinct = PCT, Crime = CRIME) %&gt;% fill(Precinct) %&gt;% filter(Crime == &quot;TOTAL SEVEN MAJOR FELONY OFFENSES&quot;) ny_crime &lt;- left_join(ny_police, ny_crime) We read in the felony data for nyc in 2021 and joined it with the sf object. Then, we create map using tmap package. Notice that in tm_polygons(), we specify the column to be 2021. ny_crime %&gt;% tm_shape() + tm_polygons(&quot;2021&quot;, palette = &quot;Blues&quot;, title=&quot;&quot;) + tm_text(&quot;Precinct&quot;, size = .65) + tm_layout(&quot;Major Felony Offenses in 2021 by\\nNYC Police Precinct&quot;, title.size = .95, frame = FALSE) For more usage, you can refer at tmap:get started! "],["cutting-room-floor.html", "Chapter 18 Cutting room floor 18.1 Parallel coordinate plots", " Chapter 18 Cutting room floor It’s worth remembering that most graphs end up on the proverbial cutting room floor. Some graph types in particular are truly hit or miss: parallel coordinate plots are at the top of the list in this category. I’m including some of the “misses” here so you’ll realize you’re not alone if you create a graph that does not show anything worthwhile. 18.1 Parallel coordinate plots library(dplyr) library(forcats) library(ggplot2) library(readr) library(stringr) library(tibble) library(tidyr) oedi_building &lt;- read_csv(&quot;data/oedi_building.csv&quot;) oedi_building |&gt; filter(str_detect(in.building_type, &quot;Office&quot;)) |&gt; GGally::ggparcoord(columns = 1:4, groupColumn = 5, alphaLines = .5, splineFactor = 10) oedi_building |&gt; select(c(starts_with(&quot;in.week&quot;)), in.heating_fuel) |&gt; rownames_to_column(&quot;ID&quot;) |&gt; pivot_longer(cols = starts_with(&quot;in.week&quot;), names_to = &quot;variable&quot;, values_to = &quot;value&quot;) |&gt; ggplot(aes(x = variable, y = value, group = ID, color = in.heating_fuel)) + geom_line(lwd = .1) + theme_bw() + coord_flip() # https://collegescorecard.ed.gov/data df &lt;- read_csv(&quot;data/college_scorecard.csv&quot;) df |&gt; na.omit() |&gt; mutate(COMP_ORIG_YR4_RT = as.numeric(COMP_ORIG_YR4_RT)) |&gt; mutate(WOMENONLY = fct_recode(factor(WOMENONLY), `Women only` = &quot;0&quot;, `Not women only` = &quot;1&quot;)) |&gt; GGally::ggparcoord(columns = 1:4, alphaLines = .5, scale = &quot;globalminmax&quot;, groupColumn = 5) + theme_bw() + theme(legend.position = &quot;bottom&quot;, legend.title = element_blank()) uk &lt;- read_csv(&quot;data/uk_universities.csv&quot;, col_types = &quot;ccdcddddddnncccccdddc&quot;) uk |&gt; GGally::ggparcoord(columns = c(5, 7, 9:12), alphaLines = .5, groupColumn = 2) + coord_flip() # https://www.strava.com/clubs/537620/leaderboard run &lt;- read_csv(&quot;data/leaderboard.csv&quot;, na = &quot;--&quot;) run$Longest &lt;- parse_number(run$Longest) run$Distance &lt;- parse_number(run$Distance) run$Pace &lt;- parse_number(run$`Avg. Pace`) run$Gain &lt;- parse_number(run$`Elev. Gain`) run |&gt; filter(Distance &lt; 1000, Rank &lt;= 50) |&gt; select(Athlete, Distance, Runs, Longest, Pace, Gain) |&gt; parcoords::parcoords(rownames = F, reorderable = TRUE, brushMode = &quot;1D-axes&quot;) library(parcoords) uk |&gt; filter(str_detect(Region, &quot;England&quot;)) |&gt; select(University_name, UK_rank, UGfees = `UG_average_fees_(in_pounds)`, PGfees = `PG_average_fees_(in_pounds)`, International_students, Student_satisfaction, COL = `Estimated_cost_of_living_per_year_(in_pounds)`, Campus_setting) |&gt; parcoords(rownames = FALSE, reorderable = TRUE, brushMode = &quot;1D-axes&quot;, color = list(colorBy = &quot;Campus_setting&quot;, colorScale = &quot;scaleOrdinal&quot;, colorScheme = &quot;schemeCategory10&quot;), withD3 = TRUE, width = 770, height = 600) "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
